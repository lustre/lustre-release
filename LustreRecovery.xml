<?xml version='1.0' encoding='UTF-8'?>
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xl="http://www.w3.org/1999/xlink" version="5.0" xml:lang="en-US"
 xml:id="lustrerecovery">
  <title xml:id="lustrerecovery.title">Lustre File System Recovery</title>
  <para>This chapter describes how recovery is implemented in a Lustre file system and includes the
    following sections:</para>
  <itemizedlist>
    <listitem>
      <para><xref linkend="recoveryoverview"/></para>
    </listitem>
    <listitem>
      <para><xref linkend="metadatereplay"/></para>
    </listitem>
    <listitem>
      <para><xref linkend="replyreconstruction"/></para>
    </listitem>
    <listitem>
      <para><xref linkend="versionbasedrecovery"/></para>
    </listitem>
    <listitem>
      <para><xref linkend="commitonshare"/></para>
    </listitem>
    <listitem>
      <para><xref linkend="imperativerecovery"/></para>
    </listitem>
  </itemizedlist>
  <section xml:id="recoveryoverview">
      <title>
          <indexterm><primary>recovery</primary></indexterm>
          <indexterm><primary>recovery</primary><secondary>VBR</secondary><see>version-based recovery</see></indexterm>
          <indexterm><primary>recovery</primary><secondary>commit on share</secondary><see>commit on share</see></indexterm>
          <indexterm><primary>lustre</primary><secondary>recovery</secondary><see>recovery</see></indexterm>
          Recovery Overview</title>
    <para>The recovery feature provided in the Lustre software is responsible for dealing with node
      or network failure and returning the cluster to a consistent, performant state. Because the
      Lustre software allows servers to perform asynchronous update operations to the on-disk file
      system (i.e., the server can reply without waiting for the update to synchronously commit to
      disk), the clients may have state in memory that is newer than what the server can recover
      from disk after a crash.</para>
    <para>A handful of different types of failures can cause recovery to occur:</para>
    <itemizedlist>
      <listitem>
        <para> Client (compute node) failure</para>
      </listitem>
      <listitem>
        <para> MDS failure (and failover)</para>
      </listitem>
      <listitem>
        <para> OST failure (and failover)</para>
      </listitem>
      <listitem>
        <para> Transient network partition</para>
      </listitem>
    </itemizedlist>
    <para>For Lustre, all Lustre file system failure and recovery operations
      are based on the concept of connection failure; all imports or exports
      associated with a given connection are considered to fail if any of
      them fail.  The <xref linkend="imperativerecovery"/> feature allows
      the MGS to actively inform clients when a target restarts after a
      failure, failover, or other interruption to speed up recovery.</para>
    <para>For information on Lustre file system recovery, see
      <xref linkend="metadatereplay"/>. For information on recovering from a
      corrupt file system, see <xref linkend="commitonshare"/>. For
      information on resolving orphaned objects, a common issue after recovery,
      see <xref linkend="dbdoclet.50438225_13916"/>. For information on
      imperative recovery see <xref linkend="imperativerecovery"/>
    </para>
    <section remap="h3">
      <title><indexterm><primary>recovery</primary><secondary>client failure</secondary></indexterm>Client Failure</title>
      <para>Recovery from client failure in a Lustre file system is based on lock revocation and
        other resources, so surviving clients can continue their work uninterrupted. If a client
        fails to timely respond to a blocking lock callback from the Distributed Lock Manager (DLM)
        or fails to communicate with the server in a long period of time (i.e., no pings), the
        client is forcibly removed from the cluster (evicted). This enables other clients to acquire
        locks blocked by the dead client&apos;s locks, and also frees resources (file handles,
        export data) associated with that client. Note that this scenario can be caused by a network
        partition, as well as an actual client node system failure. <xref linkend="networkpartition"
        /> describes this case in more detail.</para>
    </section>
    <section xml:id="clientevictions">
      <title><indexterm><primary>recovery</primary><secondary>client eviction</secondary></indexterm>Client Eviction</title>
      <para>If a client is not behaving properly from the server&apos;s point of view, it will be evicted. This ensures that the whole file system can continue to function in the presence of failed or misbehaving clients. An evicted client must invalidate all locks, which in turn, results in all cached inodes becoming invalidated and all cached data being flushed.</para>
      <para>Reasons why a client might be evicted:</para>
      <itemizedlist>
        <listitem>
          <para>Failure to respond to a server request in a timely manner</para>
          <itemizedlist>
            <listitem>
              <para>Blocking lock callback (i.e., client holds lock that another client/server wants)</para>
            </listitem>
            <listitem>
              <para>Lock completion callback (i.e., client is granted lock previously held by another client)</para>
            </listitem>
            <listitem>
              <para>Lock glimpse callback (i.e., client is asked for size of object by another client)</para>
            </listitem>
            <listitem>
              <para>Server shutdown notification (with simplified interoperability)</para>
            </listitem>
          </itemizedlist>
        </listitem>
        <listitem>
          <para>Failure to ping the server in a timely manner, unless the server is receiving no RPC traffic at all (which may indicate a network partition).</para>
        </listitem>
      </itemizedlist>
    </section>
    <section remap="h3">
      <title><indexterm><primary>recovery</primary><secondary>MDS failure</secondary></indexterm>MDS Failure (Failover)</title>
      <para>Highly-available (HA) Lustre file system operation requires that the metadata server
        have a peer configured for failover, including the use of a shared storage device for the
        MDT backing file system. The actual mechanism for detecting peer failure, power off
        (STONITH) of the failed peer (to prevent it from continuing to modify the shared disk), and
        takeover of the Lustre MDS service on the backup node depends on external HA software such
        as Heartbeat. It is also possible to have MDS recovery with a single MDS node. In this case,
        recovery will take as long as is needed for the single MDS to be restarted.</para>
      <para>When <xref linkend="imperativerecovery"/> is enabled, clients are notified of an MDS restart (either the backup or a restored primary). Clients always may detect an MDS failure either by timeouts of in-flight requests or idle-time ping messages. In either case the clients then connect to the new backup MDS and use the Metadata Replay protocol. Metadata Replay is responsible for ensuring that the backup MDS re-acquires state resulting from transactions whose effects were made visible to clients, but which were not committed to the disk.</para>
      <para>The reconnection to a new (or restarted) MDS is managed by the file system configuration loaded by the client when the file system is first mounted. If a failover MDS has been configured (using the <literal>--failnode=</literal> option to <literal>mkfs.lustre</literal> or <literal>tunefs.lustre</literal>), the client tries to reconnect to both the primary and backup MDS until one of them responds that the failed MDT is again available. At that point, the client begins recovery. For more information, see <xref linkend="metadatereplay"/>.</para>
      <para>Transaction numbers are used to ensure that operations are
      replayed in the order they were originally performed, so that they
      are guaranteed to succeed and present the same file system state as
      before the failure. In addition, clients inform the new server of their
      existing lock state (including locks that have not yet been granted).
      All metadata and lock replay must complete before new, non-recovery
      operations are permitted. In addition, only clients that were connected
      at the time of MDS failure are permitted to reconnect during the recovery
      window, to avoid the introduction of state changes that might conflict
      with what is being replayed by previously-connected clients.</para>
      <para>If multiple MDTs are in use, active-active failover
      is possible (e.g. two MDS nodes, each actively serving one or more
      different MDTs for the same filesystem). See
      <xref linkend="dbdoclet.mdtactiveactive"/> for more information.</para>
    </section>
    <section remap="h3">
      <title><indexterm><primary>recovery</primary><secondary>OST failure</secondary></indexterm>OST Failure (Failover)</title>
	<para>When an OST fails or has communication problems with the client, the default action is that the corresponding OSC enters recovery, and I/O requests going to that OST are blocked waiting for OST recovery or failover. It is possible to administratively mark the OSC as <emphasis>inactive</emphasis> on the client, in which case file operations that involve the failed OST will return an IO error (<literal>-EIO</literal>). Otherwise, the application waits until the OST has recovered or the client process is interrupted (e.g. ,with <emphasis>CTRL-C</emphasis>).</para>
      <para>The MDS (via the LOV) detects that an OST is unavailable and skips it when assigning objects to new files. When the OST is restarted or re-establishes communication with the MDS, the MDS and OST automatically perform orphan recovery to destroy any objects that belong to files that were deleted while the OST was unavailable. For more information, see <xref linkend="troubleshootingrecovery"/> (Working with Orphaned Objects).</para>
      <para>While the OSC to OST operation recovery protocol is the same as that between the MDC and
        MDT using the Metadata Replay protocol, typically the OST commits bulk write operations to
        disk synchronously and each reply indicates that the request is already committed and the
        data does not need to be saved for recovery. In some cases, the OST replies to the client
        before the operation is committed to disk (e.g. truncate, destroy, setattr, and I/O
        operations in newer releases of the Lustre software), and normal replay and resend handling
        is done, including resending of the bulk writes. In this case, the client keeps a copy of
        the data available in memory until the server indicates that the write has committed to
        disk.</para>
      <para>To force an OST recovery, unmount the OST and then mount it again. If the OST was connected to clients before it failed, then a recovery process starts after the remount, enabling clients to reconnect to the OST and replay transactions in their queue. When the OST is in recovery mode, all new client connections are refused until the recovery finishes. The recovery is complete when either all previously-connected clients reconnect and their transactions are replayed or a client connection attempt times out. If a connection attempt times out, then all clients waiting to reconnect (and their transactions) are lost.</para>
      <note>
        <para>If you know an OST will not recover a previously-connected client (if, for example, the client has crashed), you can manually abort the recovery using this command:</para>
        <para><screen>oss# lctl --device <replaceable>lustre_device_number</replaceable> abort_recovery</screen></para>
        <para>To determine an OST&apos;s device number and device name, run the <literal>lctl dl</literal> command. Sample <literal>lctl dl</literal> command output is shown below:</para>
        <screen>7 UP obdfilter ddn_data-OST0009 ddn_data-OST0009_UUID 1159 </screen>
        <para>In this example, 7 is the OST device number. The device name is <literal>ddn_data-OST0009</literal>. In most instances, the device name can be used in place of the device number.</para>
      </note>
    </section>
    <section xml:id="networkpartition">
      <title><indexterm><primary>recovery</primary><secondary>network</secondary></indexterm>Network Partition</title>
      <para>Network failures may be transient. To avoid invoking recovery, the client tries, initially, to re-send any timed out request to the server. If the resend also fails, the client tries to re-establish a connection to the server. Clients can detect harmless partition upon reconnect if the server has not had any reason to evict the client.</para>
      <para>If a request was processed by the server, but the reply was dropped (i.e., did not arrive back at the client), the server must reconstruct the reply when the client resends the request, rather than performing the same request twice.</para>
    </section>
    <section remap="h3">
      <title><indexterm><primary>recovery</primary><secondary>failed recovery</secondary></indexterm>Failed Recovery</title>
      <para>In the case of failed recovery, a client is evicted by the server and must reconnect after having flushed its saved state related to that server, as described in <xref linkend="clientevictions"/>, above. Failed recovery might occur for a number of reasons, including:</para>
      <itemizedlist>
        <listitem>
          <para> Failure of recovery</para>
          <itemizedlist>
            <listitem>
              <para> Recovery fails if the operations of one client directly depend on the operations of another client that failed to participate in recovery. Otherwise, Version Based Recovery (VBR) allows recovery to proceed for all of the connected clients, and only missing clients are evicted.</para>
            </listitem>
            <listitem>
              <para> Manual abort of recovery</para>
            </listitem>
          </itemizedlist>
        </listitem>
        <listitem>
          <para> Manual eviction by the administrator</para>
        </listitem>
      </itemizedlist>
    </section>
  </section>
  <section xml:id="metadatereplay">
    <title><indexterm><primary>recovery</primary><secondary>metadata replay</secondary></indexterm>Metadata Replay</title>
    <para>Highly available Lustre file system operation requires that the MDS have a peer configured
      for failover, including the use of a shared storage device for the MDS backing file system.
      When a client detects an MDS failure, it connects to the new MDS and uses the metadata replay
      protocol to replay its requests.</para>
    <para>Metadata replay ensures that the failover MDS re-accumulates state resulting from transactions whose effects were made visible to clients, but which were not committed to the disk.</para>
    <section remap="h3">
      <title>XID Numbers</title>
      <para>Each request sent by the client contains an XID number, which is a client-unique, monotonically increasing 64-bit integer. The initial value of the XID is chosen so that it is highly unlikely that the same client node reconnecting to the same server after a reboot would have the same XID sequence. The XID is used by the client to order all of the requests that it sends, until such a time that the request is assigned a transaction number. The XID is also used in Reply Reconstruction to uniquely identify per-client requests at the server.</para>
    </section>
    <section remap="h3">
      <title>Transaction Numbers</title>
      <para>Each client request processed by the server that involves any state change (metadata update, file open, write, etc., depending on server type) is assigned a transaction number by the server that is a target-unique, monotonically increasing, server-wide 64-bit integer. The transaction number for each file system-modifying request is sent back to the client along with the reply to that client request. The transaction numbers allow the client and server to unambiguously order every modification to the file system in case recovery is needed.</para>
      <para>Each reply sent to a client (regardless of request type) also contains the last
        committed transaction number that indicates the highest transaction number committed to the
        file system. The <literal>ldiskfs</literal> and <literal>ZFS</literal> backing file systems that the Lustre software
        uses enforces the requirement that any earlier disk operation will always be committed to
        disk before a later disk operation, so the last committed transaction number also reports
        that any requests with a lower transaction number have been committed to disk.</para>
    </section>
    <section remap="h3">
      <title>Replay and Resend</title>
      <para>Lustre file system recovery can be separated into two distinct types of operations:
          <emphasis>replay</emphasis> and <emphasis>resend</emphasis>.</para>
      <para><emphasis>Replay</emphasis> operations are those for which the client received a reply from the server that the operation had been successfully completed. These operations need to be redone in exactly the same manner after a server restart as had been reported before the server failed. Replay can only happen if the server failed; otherwise it will not have lost any state in memory.</para>
      <para><emphasis>Resend</emphasis> operations are those for which the client never received a reply, so their final state is unknown to the client. The client sends unanswered requests to the server again in XID order, and again awaits a reply for each one. In some cases, resent requests have been handled and committed to disk by the server (possibly also having dependent operations committed), in which case, the server performs reply reconstruction for the lost reply. In other cases, the server did not receive the lost request at all and processing proceeds as with any normal request. These are what happen in the case of a network interruption. It is also possible that the server received the request, but was unable to reply or commit it to disk before failure.</para>
    </section>
    <section remap="h3">
      <title>Client Replay List</title>
      <para>All file system-modifying requests have the potential to be required for server state recovery (replay) in case of a server failure. Replies that have an assigned transaction number that is higher than the last committed transaction number received in any reply from each server are preserved for later replay in a per-server replay list. As each reply is received from the server, it is checked to see if it has a higher last committed transaction number than the previous highest last committed number. Most requests that now have a lower transaction number can safely be removed from the replay list. One exception to this rule is for open requests, which need to be saved for replay until the file is closed so that the MDS can properly reference count open-unlinked files.</para>
    </section>
    <section remap="h3">
      <title>Server Recovery</title>
      <para>A server enters recovery if it was not shut down cleanly. If, upon startup, if any client entries are in the <literal>last_rcvd</literal> file for any previously connected clients, the server enters recovery mode and waits for these previously-connected clients to reconnect and begin replaying or resending their requests. This allows the server to recreate state that was exposed to clients (a request that completed successfully) but was not committed to disk before failure.</para>
      <para>In the absence of any client connection attempts, the server waits indefinitely for the clients to reconnect. This is intended to handle the case where the server has a network problem and clients are unable to reconnect and/or if the server needs to be restarted repeatedly to resolve some problem with hardware or software. Once the server detects client connection attempts - either new clients or previously-connected clients - a recovery timer starts and forces recovery to finish in a finite time regardless of whether the previously-connected clients are available or not.</para>
      <para>If no client entries are present in the <literal>last_rcvd</literal> file, or if the administrator manually aborts recovery, the server does not wait for client reconnection and proceeds to allow all clients to connect.</para>
      <para>As clients connect, the server gathers information from each one to determine how long the recovery needs to take. Each client reports its connection UUID, and the server does a lookup for this UUID in the <literal>last_rcvd</literal> file to determine if this client was previously connected. If not, the client is refused connection and it will retry until recovery is completed. Each client reports its last seen transaction, so the server knows when all transactions have been replayed. The client also reports the amount of time that it was previously waiting for request completion so that the server can estimate how long some clients might need to detect the server failure and reconnect.</para>
      <para>If the client times out during replay, it attempts to reconnect. If the client is unable to reconnect, <literal>REPLAY</literal> fails and it returns to <literal>DISCON</literal> state. It is possible that clients will timeout frequently during <literal>REPLAY</literal>, so reconnection should not delay an already slow process more than necessary. We can mitigate this by increasing the timeout during replay.</para>
    </section>
    <section remap="h3">
      <title>Request Replay</title>
      <para>If a client was previously connected, it gets a response from the server telling it that the server is in recovery and what the last committed transaction number on disk is. The client can then iterate through its replay list and use this last committed transaction number to prune any previously-committed requests. It replays any newer requests to the server in transaction number order, one at a time, waiting for a reply from the server before replaying the next request.</para>
      <para>Open requests that are on the replay list may have a transaction number lower than the server&apos;s last committed transaction number. The server processes those open requests immediately. The server then processes replayed requests from all of the clients in transaction number order, starting at the last committed transaction number to ensure that the state is updated on disk in exactly the same manner as it was before the crash. As each replayed request is processed, the last committed transaction is incremented. If the server receives a replay request from a client that is higher than the current last committed transaction, that request is put aside until other clients provide the intervening transactions. In this manner, the server replays requests in the same sequence as they were previously executed on the server until either all clients are out of requests to replay or there is a gap in a sequence.</para>
    </section>
    <section remap="h3">
      <title>Gaps in the Replay Sequence</title>
      <para>In some cases, a gap may occur in the reply sequence. This might be caused by lost replies, where the request was processed and committed to disk but the reply was not received by the client. It can also be caused by clients missing from recovery due to partial network failure or client death.</para>
      <para>In the case where all clients have reconnected, but there is a gap in the replay sequence the only possibility is that some requests were processed by the server but the reply was lost. Since the client must still have these requests in its resend list, they are processed after recovery is finished.</para>
      <para>In the case where all clients have not reconnected, it is likely that the failed clients had requests that will no longer be replayed. The VBR feature is used to determine if a request following a transaction gap is safe to be replayed. Each item in the file system (MDS inode or OST object) stores on disk the number of the last transaction in which it was modified. Each reply from the server contains the previous version number of the objects that it affects. During VBR replay, the server matches the previous version numbers in the resend request against the current version number. If the versions match, the request is the next one that affects the object and can be safely replayed. For more information, see <xref linkend="versionbasedrecovery"/>.</para>
    </section>
    <section remap="h3">
      <title><indexterm><primary>recovery</primary><secondary>locks</secondary></indexterm>Lock Recovery</title>
      <para>If all requests were replayed successfully and all clients reconnected, clients then do
        lock replay locks -- that is, every client sends information about every lock it holds from
        this server and its state (whenever it was granted or not, what mode, what properties and so
        on), and then recovery completes successfully. Currently, the Lustre software does not do
        lock verification and just trusts clients to present an accurate lock state. This does not
        impart any security concerns since Lustre software release 1.x clients are trusted for other
        information (e.g. user ID) during normal operation also.</para>
      <para>After all of the saved requests and locks have been replayed, the client sends an <literal>MDS_GETSTATUS</literal> request with last-replay flag set. The reply to that request is held back until all clients have completed replay (sent the same flagged getstatus request), so that clients don&apos;t send non-recovery requests before recovery is complete.</para>
    </section>
    <section remap="h3">
      <title>Request Resend</title>
      <para>Once all of the previously-shared state has been recovered on the server (the target file system is up-to-date with client cache and the server has recreated locks representing the locks held by the client), the client can resend any requests that did not receive an earlier reply. This processing is done like normal request processing, and, in some cases, the server may do reply reconstruction.</para>
    </section>
  </section>
  <section xml:id="replyreconstruction">
    <title>Reply Reconstruction</title>
    <para>When a reply is dropped, the MDS needs to be able to reconstruct the reply when the original request is re-sent. This must be done without repeating any non-idempotent operations, while preserving the integrity of the locking system. In the event of MDS failover, the information used to reconstruct the reply must be serialized on the disk in transactions that are joined or nested with those operating on the disk.</para>
    <section remap="h3">
      <title>Required State</title>
      <para>For the majority of requests, it is sufficient for the server to store three pieces of data in the <literal>last_rcvd</literal> file:</para>
      <itemizedlist>
        <listitem>
          <para> XID of the request</para>
        </listitem>
        <listitem>
          <para> Resulting transno (if any)</para>
        </listitem>
        <listitem>
          <para> Result code (<literal>req-&gt;rq_status</literal>)</para>
        </listitem>
      </itemizedlist>
      <para>For open requests, the &quot;disposition&quot; of the open must also be stored.</para>
    </section>
    <section remap="h3">
      <title>Reconstruction of Open Replies</title>
      <para>An open reply consists of up to three pieces of information (in addition to the contents of the &quot;request log&quot;):</para>
      <itemizedlist>
        <listitem>
          <para>File handle</para>
        </listitem>
        <listitem>
          <para>Lock handle</para>
        </listitem>
        <listitem>
          <para><literal>mds_body</literal> with information about the file created (for <literal>O_CREAT</literal>)</para>
        </listitem>
      </itemizedlist>
      <para>The disposition, status and request data (re-sent intact by the client) are sufficient to determine which type of lock handle was granted, whether an open file handle was created, and which resource should be described in the <literal>mds_body</literal>.</para>
      <section remap="h5">
        <title>Finding the File Handle</title>
        <para>The file handle can be found in the XID of the request and the list of per-export open file handles. The file handle contains the resource/FID.</para>
      </section>
      <section remap="h5">
        <title>Finding the Resource/fid</title>
        <para>The file handle contains the resource/fid.</para>
      </section>
      <section remap="h5">
        <title>Finding the Lock Handle</title>
        <para>The lock handle can be found by walking the list of granted locks for the resource looking for one with the appropriate remote file handle (present in the re-sent request). Verify that the lock has the right mode (determined by performing the disposition/request/status analysis above) and is granted to the proper client.</para>
      </section>
    </section>
    <section remap="h3" condition="l28">
      <title>Multiple Reply Data per Client</title>
      <para>Since Lustre 2.8, the MDS is able to save several reply data per client. The reply data are stored in the <literal>reply_data</literal> internal file of the MDT. Additionally to the XID of the request, the transaction number, the result code and the open "disposition", the reply data contains a generation number that identifies the client thanks to the content of the <literal>last_rcvd</literal> file.</para>
    </section>
  </section>
  <section xml:id="versionbasedrecovery">
    <title><indexterm><primary>Version-based recovery (VBR)</primary></indexterm>Version-based Recovery</title>
    <para>The Version-based Recovery (VBR) feature improves Lustre file system reliability in cases
      where client requests (RPCs) fail to replay during recovery <footnote>
        <para>There are two scenarios under which client RPCs are not replayed: (1) Non-functioning
          or isolated clients do not reconnect, and they cannot replay their RPCs, causing a gap in
          the replay sequence. These clients get errors and are evicted. (2) Functioning clients
          connect, but they cannot replay some or all of their RPCs that occurred after the gap
          caused by the non-functioning/isolated clients. These clients get errors (caused by the
          failed clients). With VBR, these requests have a better chance to replay because the
          &quot;gaps&quot; are only related to specific files that the missing client(s)
          changed.</para>
      </footnote>.</para>
    <para>In pre-VBR releases of the Lustre software, if the MGS or an OST went down and then
      recovered, a recovery process was triggered in which clients attempted to replay their
      requests. Clients were only allowed to replay RPCs in serial order. If a particular client
      could not replay its requests, then those requests were lost as well as the requests of
      clients later in the sequence. The &apos;&apos;downstream&apos;&apos; clients never got to
      replay their requests because of the wait on the earlier client&apos;s RPCs. Eventually, the
      recovery period would time out (so the component could accept new requests), leaving some
      number of clients evicted and their requests and data lost.</para>
    <para>With VBR, the recovery mechanism does not result in the loss of clients or their data, because changes in inode versions are tracked, and more clients are able to reintegrate into the cluster. With VBR, inode tracking looks like this:</para>
    <itemizedlist>
      <listitem>
        <para>Each inode<footnote>
            <para>Usually, there are two inodes, a parent and a child.</para>
          </footnote> stores a version, that is, the number of the last transaction (transno) in which the inode was changed.</para>
      </listitem>
      <listitem>
        <para>When an inode is about to be changed, a pre-operation version of the inode is saved in the client&apos;s data.</para>
      </listitem>
      <listitem>
        <para>The client keeps the pre-operation inode version and the post-operation version (transaction number) for replay, and sends them in the event of a server failure.</para>
      </listitem>
      <listitem>
        <para>If the pre-operation version matches, then the request is replayed. The post-operation version is assigned on all inodes modified in the request.</para>
      </listitem>
    </itemizedlist>
    <note>
      <para>An RPC can contain up to four pre-operation versions, because several inodes can be involved in an operation. In the case of a &apos;&apos;rename&apos;&apos; operation, four different inodes can be modified.</para>
    </note>
    <para>During normal operation, the server:</para>
    <itemizedlist>
      <listitem>
        <para>Updates the versions of all inodes involved in a given operation</para>
      </listitem>
      <listitem>
        <para>Returns the old and new inode versions to the client with the reply</para>
      </listitem>
    </itemizedlist>
    <para>When the recovery mechanism is underway, VBR follows these steps:</para>
    <orderedlist>
      <listitem>
        <para>VBR only allows clients to replay transactions if the affected inodes have the same version as during the original execution of the transactions, even if there is gap in transactions due to a missed client.</para>
      </listitem>
      <listitem>
        <para>The server attempts to execute every transaction that the client offers, even if it encounters a re-integration failure.</para>
      </listitem>
      <listitem>
        <para>When the replay is complete, the client and server check if a replay failed on any transaction because of inode version mismatch. If the versions match, the client gets a successful re-integration message. If the versions do not match, then the client is evicted.</para>
      </listitem>
    </orderedlist>
    <para>VBR recovery is fully transparent to users. It may lead to slightly longer recovery times if the cluster loses several clients during server recovery.</para>
    <section remap="h3">
        <title><indexterm><primary>Version-based recovery (VBR)</primary><secondary>messages</secondary></indexterm>VBR Messages</title>
      <para>The VBR feature is built into the Lustre file system recovery functionality. It cannot
        be disabled. These are some VBR messages that may be displayed:</para>
      <screen>DEBUG_REQ(D_WARNING, req, &quot;Version mismatch during replay\n&quot;);</screen>
      <para>This message indicates why the client was evicted. No action is needed.</para>
      <screen>CWARN(&quot;%s: version recovery fails, reconnecting\n&quot;);</screen>
      <para>This message indicates why the recovery failed. No action is needed.</para>
    </section>
    <section remap="h3">
        <title><indexterm><primary>Version-based recovery (VBR)</primary><secondary>tips</secondary></indexterm>Tips for Using VBR</title>
      <para>VBR will be successful for clients which do not share data with other client. Therefore, the strategy for reliable use of VBR is to store a client&apos;s data in its own directory, where possible. VBR can recover these clients, even if other clients are lost.</para>
    </section>
  </section>
  <section xml:id="commitonshare">
    <title><indexterm><primary>commit on share</primary></indexterm>Commit on Share</title>
    <para>The commit-on-share (COS) feature makes Lustre file system recovery more reliable by
      preventing missing clients from causing cascading evictions of other clients. With COS
      enabled, if some Lustre clients miss the recovery window after a reboot or a server failure,
      the remaining clients are not evicted.</para>
    <note>
      <para>The commit-on-share feature is enabled, by default.</para>
    </note>
    <section remap="h3">
      <title><indexterm><primary>commit on share</primary><secondary>working with</secondary></indexterm>Working with Commit on Share</title>
      <para>To illustrate how COS works, let&apos;s first look at the old recovery scenario. After a service restart, the MDS would boot and enter recovery mode. Clients began reconnecting and replaying their uncommitted transactions. Clients could replay transactions independently as long as their transactions did not depend on each other (one client&apos;s transactions did not depend on a different client&apos;s transactions). The MDS is able to determine whether one transaction is dependent on another transaction via the <xref linkend="versionbasedrecovery"/> feature.</para>
      <para>If there was a dependency between client transactions (for example, creating and deleting the same file), and one or more clients did not reconnect in time, then some clients may have been evicted because their transactions depended on transactions from the missing clients. Evictions of those clients caused more clients to be evicted and so on, resulting in &quot;cascading&quot; client evictions.</para>
      <para>COS addresses the problem of cascading evictions by eliminating dependent transactions between clients. It ensures that one transaction is committed to disk if another client performs a transaction dependent on the first one. With no dependent, uncommitted transactions to apply, the clients replay their requests independently without the risk of being evicted.</para>
    </section>
    <section remap="h3">
      <title><indexterm><primary>commit on share</primary><secondary>tuning</secondary></indexterm>Tuning Commit On Share</title>
      <para>Commit on Share can be enabled or disabled using the <literal>mdt.commit_on_sharing</literal> tunable (0/1). This tunable can be set when the MDS is created (<literal>mkfs.lustre</literal>) or when the Lustre file system is active, using the <literal>lctl set/get_param</literal> or <literal>lctl conf_param</literal> commands.</para>
      <para>To set a default value for COS (disable/enable) when the file system is created, use:</para>
      <screen>--param mdt.commit_on_sharing=0/1
</screen>
      <para>To disable or enable COS when the file system is running, use:</para>
      <screen>lctl set_param mdt.*.commit_on_sharing=0/1
</screen>
      <note>
        <para>Enabling COS may cause the MDS to do a large number of synchronous disk operations, hurting performance. Placing the <literal>ldiskfs</literal> journal on a low-latency external device may improve file system performance.</para>
      </note>
    </section>
  </section>
   <section xml:id="imperativerecovery">
    <title><indexterm><primary>imperative recovery</primary></indexterm>Imperative Recovery</title>
	<para>Large-scale Lustre file system implementations have historically experienced problems
      recovering in a timely manner after a server failure. This is due to the way that clients
      detect the server failure and how the servers perform their recovery. Many of the processes
      are driven by the RPC timeout, which must be scaled with system size to prevent false
      diagnosis of server death. The purpose of imperative recovery is to reduce the recovery window
      by actively informing clients of server failure. The resulting reduction in the recovery
      window will minimize target downtime and therefore increase overall system availability.
      Imperative Recovery does not remove previous recovery mechanisms, and client timeout-based
      recovery actions can occur in a cluster when IR is enabled as each client can still
      independently disconnect and reconnect from a target. In case of a mix of IR and non-IR
      clients connecting to an OST or MDT, the server cannot reduce its recovery timeout window,
      because it cannot be sure that all clients have been notified of the server restart in a
      timely manner. Even in such mixed environments the time to complete recovery may be reduced,
      since IR-enabled clients will still be notified to reconnect to the server promptly and allow
      recovery to complete as soon as the last non-IR client detects the server failure.</para>
	<section remap="h3">
         <title><indexterm><primary>imperative recovery</primary><secondary>MGS role</secondary></indexterm>MGS role</title>
	<para>The MGS now holds additional information about Lustre targets, in the form of a Target Status
        Table. Whenever a target registers with the MGS, there is a corresponding entry in this
        table identifying the target. This entry includes NID information, and state/version
        information for the target. When a client mounts the file system, it caches a locked copy of
        this table, in the form of a Lustre configuration log. When a target restart occurs, the MGS
        revokes the client lock, forcing all clients to reload the table. Any new targets will have
        an updated version number, the client detects this and reconnects to the restarted target.
        Since successful IR notification of server restart depends on all clients being registered
        with the MGS, and there is no other node to notify clients in case of MGS restart, the MGS
        will disable IR for a period when it first starts. This interval is configurable, as shown
        in <xref linkend="imperativerecoveryparameters"/></para>
        <para>Because of the increased importance of the MGS in recovery, it is strongly recommended that the MGS node be separate from the MDS. If the MGS is co-located on the MDS node, then in case of MDS/MGS failure there will be no IR notification for the MDS restart, and clients will always use timeout-based recovery for the MDS.  IR notification would still be used in the case of OSS failure and recovery.</para>
	<para>Unfortunately, it’s impossible for the MGS to know how many clients have been successfully notified or whether a specific client has received the restarting target information. The only thing the MGS can do is tell the target that, for example, all clients are imperative recovery-capable, so it is not necessary to wait as long for all clients to reconnect. For this reason, we still require a timeout policy on the target side, but this timeout value can be much shorter than normal recovery. </para>
	</section>
	<section remap="h3" xml:id="imperativerecoveryparameters">
	<title><indexterm><primary>imperative recovery</primary><secondary>Tuning</secondary></indexterm>Tuning Imperative Recovery</title>
	<para>Imperative recovery has a default parameter set which means it can work without any extra configuration. However, the default parameter set only fits a generic configuration. The following sections discuss the configuration items for imperative recovery.</para>
	<section remap="h5">
	<title>ir_factor</title>
	<para>Ir_factor is used to control targets’ recovery window. If imperative recovery is enabled, the recovery timeout window on the restarting target is calculated by: <emphasis>new timeout = recovery_time * ir_factor / 10 </emphasis>Ir_factor must be a value in range of [1, 10]. The default value of ir_factor is 5. The following example will set imperative recovery timeout to 80% of normal recovery timeout on the target testfs-OST0000: </para>
<screen>lctl conf_param obdfilter.testfs-OST0000.ir_factor=8</screen>
		<note> <para>If this value is too small for the system, clients may be unnecessarily evicted</para> </note>
<para>You can read the current value of the parameter in the standard manner with <emphasis>lctl get_param</emphasis>:</para>
	<screen>
# lctl get_param obdfilter.testfs-OST0000.ir_factor
# obdfilter.testfs-OST0000.ir_factor=8
</screen>
	</section>
	<section remap="h5">
	<title>Disabling Imperative Recovery</title>
	<para>Imperative recovery can be disabled manually by a mount option. For example, imperative recovery can be disabled on an OST by:</para>
	<screen># mount -t lustre -onoir /dev/sda /mnt/ost1</screen>
	<para>Imperative recovery can also be disabled on the client side with the same mount option:</para>
	<screen># mount -t lustre -onoir mymgsnid@tcp:/testfs /mnt/testfs</screen>
	<note><para>When a single client is deactivated in this manner, the MGS will deactivate imperative recovery for the whole cluster. IR-enabled clients will still get notification of target restart, but targets will not be allowed to shorten the recovery window. </para></note>
	<para>You can also disable imperative recovery globally on the MGS by writing `state=disabled’ to the controlling procfs entry</para>
	<screen># lctl set_param mgs.MGS.live.testfs="state=disabled"</screen>
	<para>The above command will disable imperative recovery for file system named <emphasis>testfs</emphasis></para>
	</section>
	<section remap="h5">
	<title>Checking Imperative Recovery State - MGS</title>
	<para>You can get the imperative recovery state from the MGS. Let’s take an example and explain states of imperative recovery:</para>
<screen>
[mgs]$ lctl get_param mgs.MGS.live.testfs
...
imperative_recovery_state:
    state: full
    nonir_clients: 0
    nidtbl_version: 242
    notify_duration_total: 0.470000
    notify_duation_max: 0.041000
    notify_count: 38
</screen>
<informaltable frame="all">
	<tgroup cols="2">
	<colspec colname="c1" colwidth="50*"/>
	<colspec colname="c2" colwidth="50*"/>
	<thead>
		<row>
		<entry>
		<para><emphasis role="bold">Item</emphasis></para>
		</entry>
		<entry>
		<para><emphasis role="bold">Meaning</emphasis></para>
		</entry>
		</row>
	</thead>
	<tbody>
		<row>
		<entry>
			<para><emphasis role="bold">
			<literal>state</literal>
			</emphasis></para>
		</entry>
		<entry>
			<para><itemizedlist>
			<listitem>
			<para><emphasis role="bold">full: </emphasis>IR is working, all clients are connected and can be notified.</para>
			</listitem>
			<listitem>
			<para><emphasis role="bold">partial: </emphasis>some clients are not IR capable.</para>
			</listitem>
			<listitem>
			<para><emphasis role="bold">disabled: </emphasis>IR is disabled, no client notification.</para>
			</listitem>
			<listitem>
			<para><emphasis role="bold">startup: </emphasis>the MGS was just restarted, so not all clients may reconnect to the MGS.</para>
			</listitem>
			</itemizedlist></para>
		</entry>
		</row>
		<row>
		<entry>
			<para><emphasis role="bold">
			<literal>nonir_clients</literal>
			</emphasis></para>
		</entry>
		<entry>
			<para>Number of non-IR capable clients in the system.</para>
		</entry>
		</row>
		<row>
		<entry>
			<para><emphasis role="bold">
			<literal>nidtbl_version</literal>
			</emphasis></para>
		</entry>
		<entry>
			<para>Version number of the target status table. Client version must match MGS.</para>
		</entry>
		</row>
		<row>
		<entry>
			<para><emphasis role="bold">
			<literal>notify_duration_total</literal>
			</emphasis></para>
		</entry>
		<entry>
			<para>[Seconds.microseconds] Total time spent by MGS notifying clients</para>
		</entry>
		</row>
		<row>
		<entry>
			<para><emphasis role="bold">
			<literal>notify_duration_max</literal>
			</emphasis></para>
		</entry>
		<entry>
			<para>[Seconds.microseconds] Maximum notification time for the MGS to notify a single IR client.</para>
		</entry>
		</row>
		<row>
		<entry>
			<para><emphasis role="bold">
			<literal>notify_count</literal>
			</emphasis></para>
		</entry>
		<entry>
			<para>Number of MGS restarts - to obtain average notification time, divide <literal>notify_duration_total</literal> by <literal>notify_count</literal></para>
		</entry>
		</row>
	</tbody>
	</tgroup>
</informaltable>

	</section>
	<section remap="h5">
	<title>Checking Imperative Recovery State - client</title>
	<para>A `client’ in IR means a Lustre client or a MDT. You can get the IR state on any node which
          running client or MDT, those nodes will always have an MGC running. An example from a
          client:</para>
	<screen>
[client]$ lctl get_param mgc.*.ir_state
mgc.MGC192.168.127.6@tcp.ir_state=
imperative_recovery: ON
client_state:
    - { client: testfs-client, nidtbl_version: 242 }
	</screen>
	<para>An example from a MDT:</para>
	<screen>
mgc.MGC192.168.127.6@tcp.ir_state=
imperative_recovery: ON
client_state:
    - { client: testfs-MDT0000, nidtbl_version: 242 }
	</screen>
<informaltable frame="all">
	<tgroup cols="2">
	<colspec colname="c1" colwidth="50*"/>
	<colspec colname="c2" colwidth="50*"/>
	<thead>
		<row>
		<entry>
		<para><emphasis role="bold">Item</emphasis></para>
		</entry>
		<entry>
		<para><emphasis role="bold">Meaning</emphasis></para>
		</entry>
		</row>
	</thead>
	<tbody>
		<row>
		<entry>
			<para><emphasis role="bold">
			<literal>imperative_recovery</literal>
			</emphasis></para>
		</entry>
		<entry>
			<para><literal>imperative_recovery</literal>can be ON or OFF. If it’s OFF state, then IR is disabled by administrator at mount time. Normally this should be ON state.</para>
		</entry>
		</row>
		<row>
		<entry>
			<para><emphasis role="bold">
			<literal>client_state: client:</literal>
			</emphasis></para>
		</entry>
		<entry>
			<para>The name of the client</para>
		</entry>
		</row>
		<row>
		<entry>
			<para><emphasis role="bold">
			<literal>client_state: nidtbl_version</literal>
			</emphasis></para>
		</entry>
		<entry>
			<para>Version number of the target status table. Client version must match MGS.</para>
		</entry>
		</row>
	</tbody>
	</tgroup>
</informaltable>
	</section>
	<section remap="h5">
	<title>Target Instance Number</title>
	<para>The Target Instance number is used to determine if a client is connecting to the latest instance of a target. We use the lowest 32 bit of mount count as target instance number. For an OST you can get the target instance number of testfs-OST0001 in this way (the command is run from an OSS login prompt):</para>
<screen>
$ lctl get_param obdfilter.testfs-OST0001*.instance
obdfilter.testfs-OST0001.instance=5
</screen>
	<para>From a client, query the relevant OSC:</para>
<screen>
$ lctl get_param osc.testfs-OST0001-osc-*.import |grep instance
    instance: 5
</screen>
        </section>
        </section>
        <section remap="h3" xml:id="imperativerecoveryrecomendations">
        <title><indexterm><primary>imperative recovery</primary><secondary>Configuration Suggestions</secondary></indexterm>Configuration Suggestions for Imperative Recovery</title>
          <para>We used to build the MGS and MDT0000 on the same target to save
            a server node. However, to make IR work efficiently, we strongly
            recommend running the MGS node on a separate node for any
            significant Lustre file system installation. There are three main
            advantages of doing this: </para>
          <orderedlist>
            <listitem><para>Be able to notify clients when MDT0000 recovered.
            </para></listitem>
            <listitem><para>Improved load balance. The load on the MDS may be
              very high which may make the MGS unable to notify the clients in
              time.</para></listitem>
            <listitem><para>Robustness. The MGS code is simpler and much smaller
              compared to the MDS code. This means the chance of an MGS downtime
              due to a software bug is very low.
            </para></listitem>
            </orderedlist>
        </section>
  </section>

  <section xml:id="suppressingpings">
  <title><indexterm><primary>suppressing pings</primary></indexterm>Suppressing Pings</title>
    <para>On clusters with large numbers of clients and OSTs,
      <literal>OBD_PING</literal> messages may impose significant performance
      overheads. There is an option to suppress pings, allowing ping overheads
      to be considerably reduced. Before turning on this option, administrators
      should consider the following requirements and understand the trade-offs
      involved:</para>
    <itemizedlist>
      <listitem>
        <para>When suppressing pings, a server cannot detect client deaths,
          since clients do not send pings that are only to keep their
          connections alive. Therefore, a mechanism external to the Lustre
          file system shall be set up to notify Lustre targets of client
          deaths in a timely manner, so that stale connections do not exist
          for too long and lock callbacks to
          dead clients do not always have to wait for timeouts.</para>
      </listitem>
      <listitem>
        <para>Without pings, a client has to rely on Imperative Recovery to notify it of target failures, in order to join recoveries in time.  This dictates that the client shall eargerly keep its MGS connection alive.  Thus, a highly available standalone MGS is recommended and, on the other hand, MGS pings are always sent regardless of how the option is set.</para>
      </listitem>
      <listitem>
        <para>If a client has uncommitted requests to a target and it is not sending any new requests on the connection, it will still ping that target even when pings should be suppressed.  This is because the client needs to query the target's last committed transaction numbers in order to free up local uncommitted requests (and possibly other resources associated).  However, these pings shall stop as soon as all the uncommitted requests have been freed or new requests need to be sent, rendering the pings unnecessary.</para>
      </listitem>
    </itemizedlist>
    <section remap="h3">
    <title><indexterm><primary>pings</primary><secondary>suppress_pings</secondary></indexterm>"suppress_pings" Kernel Module Parameter</title>
      <para>The new option that controls whether pings are suppressed is implemented as the ptlrpc kernel module parameter "suppress_pings".  Setting it to "1" on a server turns on ping suppressing for all targets on that server, while leaving it with the default value "0" gives previous pinging behavior.  The parameter is ignored on clients and the MGS.  While the parameter is recommended to be set persistently via the modprobe.conf(5) mechanism, it also accept online changes through sysfs.  Note that an online change only affects connections established later; existing connections' pinging behaviors stay the same.</para>
    </section>
    <section remap="h3">
    <title><indexterm><primary>pings</primary><secondary>evict_client</secondary></indexterm>Client Death Notification</title>
      <para>The required external client death notification shall write UUIDs of dead clients into targets' "evict_client" procfs entries like</para>
      <screen>
/proc/fs/lustre/obdfilter/testfs-OST0000/evict_client
/proc/fs/lustre/obdfilter/testfs-OST0001/evict_client
/proc/fs/lustre/mdt/testfs-MDT0000/evict_client
      </screen>
      <para>Clients' UUIDs can be obtained from their "uuid" procfs entries like</para>
      <screen>
/proc/fs/lustre/llite/testfs-ffff8800612bf800/uuid
      </screen>
    </section>
  </section>

</chapter>
<!--
  vim:expandtab:shiftwidth=2:tabstop=8:
  -->
