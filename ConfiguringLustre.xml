<?xml version="1.0" encoding="UTF-8"?>
<chapter version="5.0" xml:lang="en-US" xmlns="http://docbook.org/ns/docbook" xmlns:xl="http://www.w3.org/1999/xlink" xml:id='configuringlustre'>
  <info>
    <title xml:id='configuringlustre.title'>Configuring Lustre</title>
  </info>



  <para>This chapter shows how to configure a simple Lustre system comprised of a combined MGS/MDT, an OST and a client. It includes:</para>
  <itemizedlist><listitem>
          <para><xref linkend="dbdoclet.50438267_50692"/>
          </para>
      </listitem>
      <listitem>
          <para><xref linkend="dbdoclet.50438267_76752"/>
          </para>
      </listitem>
  </itemizedlist>

    <section xml:id="dbdoclet.50438267_50692">
      <title>10.1 Configuring a Simple Lustre File System</title>
      <para>A Lustre system can be set up in a variety of configurations by using the administrative utilities provided with Lustre. The procedure below shows how to to configure a simple Lustre file system consisting of a combined MGS/MDS, one OSS with two OSTs, and a client. For an overview of the entire Lustre installation procedure, see <xref linkend='installoverview'/>.</para>
      <para>This configuration procedure assumes you have completed the following:</para>
      <itemizedlist><listitem>
              <para><emphasis>Set up and configured your hardware</emphasis> . For more information about hardware requirements, see <xref linkend='settinguplustresystem'/>.</para>
        </listitem>
<listitem>
    <para><emphasis>Downloaded and installed the Lustre software.</emphasis>  For more information about preparing for and installing the Lustre software, see <xref linkend='installinglustre'/>.</para>
        </listitem>
</itemizedlist>
      <para>The following optional steps should also be completed, if needed, before the Lustre software is configured:</para>
      <itemizedlist><listitem>
              <para><emphasis>Set up a hardware or software RAID on block devices to be used as OSTs or MDTs.</emphasis>  For information about setting up RAID, see the documentation for your RAID controller or <xref linkend='configuringstorage'/>.</para>
        </listitem>
<listitem>
    <para><emphasis>Set up network interface bonding on Ethernet interfaces.</emphasis>  For information about setting up network interface bonding, see <xref linkend='settingupbonding'/>.</para>
        </listitem>
<listitem>
          <para><emphasis>Set</emphasis>lnet<emphasis>module parameters to specify how Lustre Networking (LNET) is to be configured to work with Lustre and test the LNET configuration.</emphasis>  LNET will, by default, use the first TCP/IP interface it discovers on a system. If this network configuration is sufficient, you do not need to configure LNET. LNET configuration is required if you are using Infiniband or multiple Ethernet interfaces.</para>
        </listitem>
</itemizedlist>
<para>For information about configuring LNET, see <xref linkend='configuringlnet'/>. For information about testing LNET, see <xref linkend='lnetselftest'/>.</para>
      <itemizedlist><listitem>
              <para><emphasis>Run the benchmark script sgpdd_survey to determine baseline performance of your hardware.</emphasis>  Benchmarking your hardware will simplify debugging performance issues that are unrelated to Lustre and ensure you are getting the best possible performance with your installation. For information about running sgpdd_survey, see <xref linkend='benchmarkingtests'/>.</para>
        </listitem>
</itemizedlist>

<note>
    <para>
The sgpdd_survey script overwrites the device being tested so it must be run before the OSTs are configured.</para>
</note>

      <para>To configure a simple Lustre file system, complete these steps:</para>
      <orderedlist>
          <listitem>
      <para><emphasis role="bold">Create</emphasis> a combined MGS/MDT file system on a block device. On the MDS node, run:</para>
      <screen>mkfs.lustre --fsname=&lt;<emphasis>fsname</emphasis>&gt; --mgs --mdt &lt;<emphasis>block device name</emphasis>&gt;
</screen>

<note>
    <para>
If you plan to generate multiple file systems, the MGS should be created separately on its own dedicated block device, by running:</para><para> mkfs.lustre --fsname=&lt;<emphasis>fsname</emphasis>&gt; --mgs &lt;<emphasis>block device name</emphasis>&gt;</para>
</note>



  </listitem>
  <listitem>
       <para>Mount the combined MGS/MDT file system on the block device. On the MDS node, run:</para>
      <screen>mount -t lustre &lt;<emphasis>block device name</emphasis>&gt; &lt;<emphasis>mount point</emphasis>&gt;
</screen>

<note><para>
If you have created and MGS and an MDT on separate block devices, mount them both.</para>
</note>


  </listitem>
  <listitem xml:id="dbdoclet.50438267_pgfId-1290915">
      <para>Create the OST. On the OSS node, run:</para>
      <screen>mkfs.lustre --ost --fsname=&lt;<emphasis>fsname</emphasis>&gt; --mgsnode=&lt;<emphasis>NID</emphasis>&gt; &lt;<emphasis>block device name</emphasis>&gt;
</screen>
      <para>When you create an OST, you are formatting a ldiskfs file system on a block storage device like you would with any local file system.</para>
      <para>You can have as many OSTs per OSS as the hardware or drivers allow. For more information about storage and memory requirements for a Lustre file system, see <xref linkend='settinguplustresystem'/>.</para>
      <para>You can only configure one OST per block device. You should create an OST that uses the raw block device and does not use partitioning.</para>
      <para>If you are using block devices that are accessible from multiple OSS nodes, ensure that you mount the OSTs from only one OSS node at at time. It is strongly recommended that multiple-mount protection be enabled for such devices to prevent serious data corruption. For more information about multiple-mount protection, see <xref linkend='managingfailover'/>.</para>

      <note><para>
Lustre currently supports block devices up to 16 TB on OEL 5/RHEL 5 (up to 8 TB on other distributions). If the device size is only slightly larger that 16 TB, it is recommended that you limit the file system size to 16 TB at format time. If the size is significantly larger than 16 TB, you should reconfigure the storage into devices smaller than 16 TB. We recommend that you not place partitions on top of RAID 5/6 block devices due to negative impacts on performance.</para>
</note>


  </listitem>
  <listitem xml:id="dbdoclet.50438267_pgfId-1293955">
      <para>Mount the OST. On the OSS node where the OST was created, run:</para>
      <screen>mount -t lustre <emphasis>&lt;block device name&gt; &lt;mount point&gt;</emphasis></screen>


      <note><para>
              To create additional OSTs, repeat <xref linkend='dbdoclet.50438267_pgfId-1290915'/>Step 3 and <xref linkend='dbdoclet.50438267_pgfId-1293955'/>Step 4.</para>
</note>

  </listitem>
  <listitem xml:id="dbdoclet.50438267_pgfId-1290934">
       <para>Mount the Lustre file system on the client. On the client node, run:</para>
      <screen>mount -t lustre &lt;<emphasis>MGS node</emphasis>&gt;:/&lt;<emphasis>fsname</emphasis>&gt; &lt;<emphasis>mount point</emphasis>&gt; 
</screen>

<note><para>
        To create additional clients, repeat <xref linkend='dbdoclet.50438267_pgfId-1290934'/>Step 5.</para>
</note>

  </listitem>
  <listitem>
       <para>Verify that the file system started and is working correctly. Do this by running the lfs df, dd and ls commands on the client node.</para>

       <note><para>
If you have a problem mounting the file system, check the syslogs on the client and all the servers for errors and also check the network settings. A common issue with newly-installed systems is that hosts.deny or firewall rules may prevent connections on port 988.</para>
</note>


  </listitem>
  <listitem>
       <para><emphasis>(Optional) Run benchmarking to</emphasis>ols to validate the performance of hardware and software layers in the cluster. Available tools include:</para>

      <itemizedlist><listitem>
              <para>obdfilter_survey  - Characterizes the storage performance of a Lustre file system. For details, see <xref linkend='benchmarkingtests'/><emphasis><link xl:href="BenchmarkingTests.html#50438212_40624">Testing OST Performance (obdfilter_survey)</link></emphasis>.</para>
        </listitem>
<listitem>
          <para>ost_survey  - Performs I/O against OSTs to detect anomalies between otherwise identical disk subsystems. For details, see <xref linkend='benchmarkingtests'/><emphasis><link xl:href="BenchmarkingTests.html#50438212_85136">Testing OST I/O Performance (ost_survey)</link></emphasis>.</para>
        </listitem>
</itemizedlist>
        </listitem>
</orderedlist>

      <section remap="h3">
        <title>10.1.1 Simple Lustre <anchor xml:id="dbdoclet.50438267_marker-1290955" xreflabel=""/>Configuration Example</title>
        <para>To see the steps in a simple Lustre configuration, follow this example in which a combined MGS/MDT and two OSTs are created. Three block devices are used, one for the combined MGS/MDS node and one for each OSS node. Common parameters used in the example are listed below, along with individual node parameters.</para>
        <informaltable frame="all">
          <tgroup cols="4">
            <colspec colname="c1" colwidth="2*"/>
            <colspec colname="c2" colwidth="25*"/>
            <colspec colname="c3" colwidth="25*"/>
            <colspec colname="c4" colwidth="25*"/>
            <thead>
              <row>
                <entry nameend="c2" namest="c1"><para><emphasis role="bold">Common Parameters</emphasis></para></entry>
                <entry><para><emphasis role="bold">Value</emphasis></para></entry>
                <entry><para><emphasis role="bold">Description</emphasis></para></entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry><para>  </para></entry>
                <entry><para> <emphasis role="bold">MGS node</emphasis></para></entry>
                <entry><para> 10.2.0.1@tcp0</para></entry>
                <entry><para> Node for the combined MGS/MDS</para></entry>
              </row>
              <row>
                <entry><para>  </para></entry>
                <entry><para> <emphasis role="bold">file system</emphasis></para></entry>
                <entry><para> temp</para></entry>
                <entry><para> Name of the Lustre file system</para></entry>
              </row>
              <row>
                <entry><para>  </para></entry>
                <entry><para> <emphasis role="bold">network type</emphasis></para></entry>
                <entry><para> TCP/IP</para></entry>
                <entry><para> Network type used for Lustre file system temp</para></entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable>
          <informaltable frame="all">
          <tgroup cols="4">
            <colspec colname="c1" colwidth="25*"/>
            <colspec colname="c2" colwidth="25*"/>
            <colspec colname="c3" colwidth="25*"/>
            <colspec colname="c4" colwidth="25*"/>
            <thead>
              <row>
                <entry nameend="c2" namest="c1"><para><emphasis role="bold">Node Parameters</emphasis></para></entry>
                <entry><para><emphasis role="bold">Value</emphasis></para></entry>
                <entry><para><emphasis role="bold">Description</emphasis></para></entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry nameend="c4" namest="c1"><para> MGS/MDS node</para></entry>
              </row>
              <row>
                <entry><para>  </para></entry>
                <entry><para> <emphasis role="bold">MGS/MDS node</emphasis></para></entry>
                <entry><para> mdt1</para></entry>
                <entry><para> MDS in Lustre file system temp</para></entry>
              </row>
              <row>
                <entry><para>  </para></entry>
                <entry><para> <emphasis role="bold">block device</emphasis></para></entry>
                <entry><para> /dev/sdb</para></entry>
                <entry><para> Block device for the combined MGS/MDS node</para></entry>
              </row>
              <row>
                <entry><para>  </para></entry>
                <entry><para> <emphasis role="bold">mount point</emphasis></para></entry>
                <entry><para> /mnt/mdt</para></entry>
                <entry><para> Mount point for the mdt1 block device (/dev/sdb) on the MGS/MDS node</para></entry>
              </row>
              <row>
                <entry nameend="c4" namest="c1"><para> First OSS node</para></entry>
              </row>
              <row>
                <entry><para>  </para></entry>
                <entry><para> <emphasis role="bold">OSS node</emphasis></para></entry>
                <entry><para> oss1</para></entry>
                <entry><para> First OSS node in Lustre file system temp</para></entry>
              </row>
              <row>
                <entry><para>  </para></entry>
                <entry><para> <emphasis role="bold">OST</emphasis></para></entry>
                <entry><para> ost1</para></entry>
                <entry><para> First OST in Lustre file system temp</para></entry>
              </row>
              <row>
                <entry><para>  </para></entry>
                <entry><para> <emphasis role="bold">block device</emphasis></para></entry>
                <entry><para> /dev/sdc</para></entry>
                <entry><para> Block device for the first OSS node (oss1)</para></entry>
              </row>
              <row>
                <entry><para>  </para></entry>
                <entry><para> <emphasis role="bold">mount point</emphasis></para></entry>
                <entry><para> /mnt/ost1</para></entry>
                <entry><para> Mount point for the ost1 block device (/dev/sdc) on the oss1 node</para></entry>
              </row>
              <row>
                <entry nameend="c4" namest="c1"><para> Second OSS node</para></entry>
              </row>
              <row>
                <entry><para>  </para></entry>
                <entry><para> <emphasis role="bold">OSS node</emphasis></para></entry>
                <entry><para> oss2</para></entry>
                <entry><para> Second OSS node in Lustre file system temp</para></entry>
              </row>
              <row>
                <entry><para>  </para></entry>
                <entry><para> <emphasis role="bold">OST</emphasis></para></entry>
                <entry><para> ost2</para></entry>
                <entry><para> Second OST in Lustre file system temp</para></entry>
              </row>
              <row>
                <entry><para>  </para></entry>
                <entry><para> <emphasis role="bold">block device</emphasis></para></entry>
                <entry><para> /dev/sdd</para></entry>
                <entry><para> Block device for the second OSS node (oss2)</para></entry>
              </row>
              <row>
                <entry><para>  </para></entry>
                <entry><para> <emphasis role="bold">mount point</emphasis></para></entry>
                <entry><para> /mnt/ost2</para></entry>
                <entry><para> Mount point for the ost2 block device (/dev/sdd) on the oss2 node</para></entry>
              </row>
              <row>
                <entry nameend="c4" namest="c1"><para> Client node</para></entry>
              </row>
              <row>
                <entry><para>  </para></entry>
                <entry><para> <emphasis role="bold">client node</emphasis></para></entry>
                <entry><para> client1</para></entry>
                <entry><para> Client in Lustre file system temp</para></entry>
              </row>
              <row>
                <entry><para>  </para></entry>
                <entry><para> <emphasis role="bold">mount point</emphasis></para></entry>
                <entry><para> /lustre</para></entry>
                <entry><para> Mount point for Lustre file system temp on the client1 node</para></entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable>



        <note>
            <para>
We recommend that you use 'dotted-quad' notation for IP addresses rather than host names to make it easier to read debug logs and debug configurations with multiple interfaces.</para>
         </note>

        <para>For this example, complete the steps below:</para>

        <orderedlist>
            <listitem>

        <para>Create a combined MGS/MDT file system on the block device. On the MDS node, run:</para>
        <screen>[root@mds /]# mkfs.lustre --fsname=temp --mgs --mdt /dev/sdb
</screen>
        <para>This command generates this output:</para>
        <screen>    Permanent disk data:
Target:            temp-MDTffff
Index:             unassigned
Lustre FS: temp
Mount type:        ldiskfs
Flags:             0x75
   (MDT MGS needs_index first_time update )
Persistent mount opts: errors=remount-ro,iopen_nopriv,user_xattr
Parameters: mdt.group_upcall=/usr/sbin/l_getgroups
 
checking for existing Lustre data: not found
device size = 16MB
2 6 18
formatting backing filesystem ldiskfs on /dev/sdb
   target name             temp-MDTffff
   4k blocks               0
   options                 -i 4096 -I 512 -q -O dir_index,uninit_groups -F
mkfs_cmd = mkfs.ext2 -j -b 4096 -L temp-MDTffff  -i 4096 -I 512 -q -O 
dir_index,uninit_groups -F /dev/sdb
Writing CONFIGS/mountdata 
</screen>
            </listitem>
            <listitem>

        <para>Mount the combined MGS/MDT file system on the block device. On the MDS node, run:</para>
        <screen>[root@mds /]# mount -t lustre /dev/sdb /mnt/mdt
</screen>
        <para>This command generates this output:</para>
        <screen>Lustre: temp-MDT0000: new disk, initializing 
Lustre: 3009:0:(lproc_mds.c:262:lprocfs_wr_group_upcall()) temp-MDT0000: gr\
oup upcall set to /usr/sbin/l_getgroups
Lustre: temp-MDT0000.mdt: set parameter group_upcall=/usr/sbin/l_getgroups
Lustre: Server temp-MDT0000 on device /dev/sdb has started 
</screen>
            </listitem>
            <listitem xml:id="dbdoclet.50438267_pgfId-1291170">
        <para>Create and mount ost1.</para>
        <para>In this example, the OSTs (ost1 and ost2) are being created on different OSSs (oss1 and oss2 respectively).</para>
            <orderedlist>
            <listitem>
        <para>a. Create ost1. On oss1 node, run:</para>
        <screen>[root@oss1 /]# mkfs.lustre --ost --fsname=temp --mgsnode=10.2.0.1@tcp0 /dev\
/sdc
</screen>
        <para>The command generates this output:</para>
        <screen>    Permanent disk data:
Target:            temp-OSTffff
Index:             unassigned
Lustre FS: temp
Mount type:        ldiskfs
Flags:             0x72
(OST needs_index first_time update)
Persistent mount opts: errors=remount-ro,extents,mballoc
Parameters: mgsnode=10.2.0.1@tcp
 
checking for existing Lustre data: not found
device size = 16MB
2 6 18
formatting backing filesystem ldiskfs on /dev/sdc
   target name             temp-OSTffff
   4k blocks               0
   options                 -I 256 -q -O dir_index,uninit_groups -F
mkfs_cmd = mkfs.ext2 -j -b 4096 -L temp-OSTffff  -I 256 -q -O
dir_index,uninit_groups -F /dev/sdc
Writing CONFIGS/mountdata 
</screen>
            </listitem>
            <listitem>
        <para>b. Mount ost1 on the OSS on which it was created. On oss1 node, run:</para>
        <screen>root@oss1 /] mount -t lustre /dev/sdc /mnt/ost1 
</screen>
        <para>The command generates this output:</para>
        <screen>LDISKFS-fs: file extents enabled 
LDISKFS-fs: mballoc enabled
Lustre: temp-OST0000: new disk, initializing
Lustre: Server temp-OST0000 on device /dev/sdb has started
</screen>
        <para>Shortly afterwards, this output appears:</para>
        <screen>Lustre: temp-OST0000: received MDS connection from 10.2.0.1@tcp0
Lustre: MDS temp-MDT0000: temp-OST0000_UUID now active, resetting orphans 
</screen>
            </listitem>
        </orderedlist>
            </listitem>
            <listitem>
        <para>Create and mount ost2.</para>
            <orderedlist>
            <listitem>
        <para>Create ost2. On oss2 node, run:</para>
        <screen>[root@oss2 /]# mkfs.lustre --ost --fsname=temp --mgsnode=10.2.0.1@tcp0 /dev\
/sdd
</screen>
        <para>The command generates this output:</para>
        <screen>    Permanent disk data:
Target:            temp-OSTffff
Index:             unassigned
Lustre FS: temp
Mount type:        ldiskfs
Flags:             0x72
(OST needs_index first_time update)
Persistent mount opts: errors=remount-ro,extents,mballoc
Parameters: mgsnode=10.2.0.1@tcp
 
checking for existing Lustre data: not found
device size = 16MB
2 6 18
formatting backing filesystem ldiskfs on /dev/sdd
   target name             temp-OSTffff
   4k blocks               0
   options                 -I 256 -q -O dir_index,uninit_groups -F
mkfs_cmd = mkfs.ext2 -j -b 4096 -L temp-OSTffff  -I 256 -q -O
dir_index,uninit_groups -F /dev/sdc
Writing CONFIGS/mountdata 
</screen>
            </listitem>
            <listitem>
        <para>Mount ost2 on the OSS on which it was created. On oss2 node, run:</para>
        <screen>root@oss2 /] mount -t lustre /dev/sdd /mnt/ost2 
</screen>
        <para>The command generates this output:</para>
        <screen>LDISKFS-fs: file extents enabled 
LDISKFS-fs: mballoc enabled
Lustre: temp-OST0000: new disk, initializing
Lustre: Server temp-OST0000 on device /dev/sdb has started
</screen>
        <para>Shortly afterwards, this output appears:</para>
        <screen>Lustre: temp-OST0000: received MDS connection from 10.2.0.1@tcp0
Lustre: MDS temp-MDT0000: temp-OST0000_UUID now active, resetting orphans 
</screen>
            </listitem>
    </orderedlist>
            </listitem>
            <listitem>
        <para>Mount the Lustre file system on the client. On the client node, run:</para>
        <screen>root@client1 /] mount -t lustre 10.2.0.1@tcp0:/temp /lustre 
</screen>
        <para>This command generates this output:</para>
        <screen>Lustre: Client temp-client has started
</screen>
            </listitem>
            <listitem>
        <para>Verify that the file system started and is working by running the df, dd and ls commands on the client node.</para>
<orderedlist>
<listitem>
        <para>Run the lfsdf -h command:</para>
        <screen>[root@client1 /] lfs df -h 
</screen>
        <para>The lfsdf-h command lists space usage per OST and the MDT in human-readable format. This command generates output similar to this:</para>
        <screen>UUID                        bytes           Used            Available      \
 Use%    Mounted on
temp-MDT0000_UUID  8.0G            400.0M          7.6G            0%      \
/lustre[MDT:0]
temp-OST0000_UUID  800.0G          400.0M          799.6G          0%      \
/lustre[OST:0]
temp-OST0001_UUID  800.0G          400.0M          799.6G          0%      \
/lustre[OST:1]
filesystem summary:        1.6T            800.0M          1.6T            \
0%      /lustre
 
</screen>
</listitem>
<listitem>
        <para>Run the lfsdf-ih command.</para>
        <screen>[root@client1 /] lfs df -ih
</screen>
        <para>The lfsdf-ih command lists inode usage per OST and the MDT. This command generates output similar to this:</para>
        <screen>UUID                        Inodes       IUsed      IFree   IUse%   Mounted\
 on
temp-MDT0000_UUID  2.5M         32         2.5M    0%      /lustre[MDT:0]
temp-OST0000_UUID  5.5M         54         5.5M    0%      /lustre[OST:0]
temp-OST0001_UUID  5.5M         54         5.5M    0%      /lustre[OST:1]
filesystem summary:        2.5M         32         2.5M    0%      /lustre
 
</screen>
</listitem>
<listitem>
        <para>c. Run the dd command:</para>
        <screen>[root@client1 /] cd /lustre
[root@client1 /lustre] dd if=/dev/zero of=/lustre/zero.dat bs=4M count=2
</screen>
        <para>The dd command verifies write functionality by creating a file containing all zeros (0s). In this command, an 8 MB file is created. This command generates output similar to this:</para>
        <screen>2+0 records in
2+0 records out
8388608 bytes (8.4 MB) copied, 0.159628 seconds, 52.6 MB/s
</screen>

</listitem>
<listitem>

        <para>d. Run the ls command:</para>
        <screen>[root@client1 /lustre] ls -lsah
</screen>
        <para>The ls-lsah command lists files and directories in the current working directory. This command generates output similar to this:</para>
        <screen>total 8.0M
4.0K drwxr-xr-x  2 root root 4.0K Oct 16 15:27 .
8.0K drwxr-xr-x 25 root root 4.0K Oct 16 15:27 ..
8.0M -rw-r--r--  1 root root 8.0M Oct 16 15:27 zero.dat 
 
</screen>
            </listitem>
        </orderedlist>
            </listitem>
        </orderedlist>
        <para>Once the Lustre file system is configured, it is ready for use.</para>
      </section>
    </section>
    <section xml:id="dbdoclet.50438267_76752">
      <title>10.2 Additional Configuration Options</title>
      <para>This section describes how to scale the Lustre file system or make configuration changes using the Lustre configuration utilities.</para>
      <section remap="h3">
        <title>10.2.1 Scaling the <anchor xml:id="dbdoclet.50438267_marker-1292440" xreflabel=""/>Lustre File System</title>
        <para>A Lustre file system can be scaled by adding OSTs or clients. For instructions on creating additional OSTs repeat <xref linkend="dbdoclet.50438267_pgfId-1291170"/>Step 3 and <xref linkend="dbdoclet.50438267_pgfId-1293955"/>Step 4 above. For mounting additional clients, repeat <xref linkend="dbdoclet.50438267_pgfId-1290934"/>Step 5 for each client.</para>
      </section>
      <section remap="h3">
        <title>10.2.2 <anchor xml:id="dbdoclet.50438267_50212" xreflabel=""/>Changing Striping Defaults</title>
        <para>The default settings for the file layout stripe pattern are shown in <xref linkend='configuringlustre.tab.stripe'/>.</para>
        <table frame="none" xml:id='configuringlustre.tab.stripe'>
          <title>Default stripe pattern</title>
          <tgroup cols="3">
            <colspec colname="c1" colwidth="3*"/>
            <colspec colname="c2" colwidth="13*"/>
            <colspec colname="c3" colwidth="13*"/>
            <tbody>
              <row>
                <entry><para><emphasis role="bold">File Layout Parameter</emphasis></para></entry>
                <entry><para><emphasis role="bold">Default</emphasis></para></entry>
                <entry><para><emphasis role="bold">Description</emphasis></para></entry>
              </row>
              <row>
                <entry><para> stripe_size</para></entry>
                <entry><para> 1 MB</para></entry>
                <entry><para> Amount of data to write to one OST before moving to the next OST.</para></entry>
              </row>
              <row>
                <entry><para> stripe_count</para></entry>
                <entry><para> 1</para></entry>
                <entry><para> The number of OSTs to use for a single file.</para></entry>
              </row>
              <row>
                <entry><para> start_ost</para></entry>
                <entry><para> -1</para></entry>
                <entry><para> The first OST where objects are created for each file. The default -1 allows the MDS to choose the starting index based on available space and load balancing. <emphasis>It's strongly recommended not to change the default for this parameter to a value other than -1.</emphasis></para></entry>
              </row>
            </tbody>
          </tgroup>
        </table>
        <para>Use the lfs setstripe command described in <xref linkend='managingstripingfreespace'/> to change the file layout configuration.</para>
      </section>
      <section remap="h3">
        <title>10.2.3 Using the Lustre Configuration Utilities</title>
        <para>If additional configuration is necessary, several configuration utilities are available:</para>
        <itemizedlist><listitem>
            <para>mkfs.lustre  - Use to format a disk for a Lustre service.</para>
          </listitem>
<listitem>
            <para>tunefs.lustre  - Use to modify configuration information on a Lustre target disk.</para>
          </listitem>
<listitem>
            <para>lctl  - Use to directly control Lustre via an ioctl interface, allowing various configuration, maintenance and debugging features to be accessed.</para>
          </listitem>
<listitem>
            <para>mount.lustre  - Use to start a Lustre client or target service.</para>
          </listitem>
</itemizedlist>
<para>For examples using these utilities, see the topic <xref linkend='systemconfigurationutilities'/></para>
        <para>The lfs utility is usful for configuring and querying a variety of options related to files. For more information, see <xref linkend='userutilities'/>.</para>

        <note><para>
Some sample scripts are included in the directory where Lustre is installed. If you have installed the Lustre source code, the scripts are located in the lustre/tests sub-directory. These scripts enable quick setup of some simple standard Lustre configurations.</para>
</note>

      </section>
  </section>
</chapter>
