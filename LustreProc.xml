<?xml version='1.0' encoding='UTF-8'?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xl="http://www.w3.org/1999/xlink" version="5.0"
  xml:lang="en-US" xml:id="lustreproc">
  <title xml:id="lustreproc.title">Lustre Parameters</title>
  <para>The <literal>/proc</literal> and <literal>/sys</literal> file systems
  acts as an interface to internal data structures in the kernel. This chapter
  describes parameters and tunables that are useful for optimizing and
  monitoring aspects of a Lustre file system. It includes these sections:</para>
  <itemizedlist>
    <listitem>
      <para><xref linkend="dbdoclet.50438271_83523"/></para>
      <para>.</para>
    </listitem>
  </itemizedlist>
  <section>
    <title>Introduction to Lustre Parameters</title>
    <para>Lustre parameters and statistics files provide an interface to
    internal data structures in the kernel that enables monitoring and
    tuning of many aspects of Lustre file system and application performance.
    These data structures include settings and metrics for components such
    as memory, networking, file systems, and kernel housekeeping routines,
    which are available throughout the hierarchical file layout.
    </para>
    <para>Typically, metrics are accessed via <literal>lctl get_param</literal>
    files and settings are changed by via <literal>lctl set_param</literal>.
    Some data is server-only, some data is client-only, and some data is
    exported from the client to the server and is thus duplicated in both
    locations.</para>
    <note>
      <para>In the examples in this chapter, <literal>#</literal> indicates
      a command is entered as root.  Lustre servers are named according to the
      convention <literal><replaceable>fsname</replaceable>-<replaceable>MDT|OSTnumber</replaceable></literal>.
        The standard UNIX wildcard designation (*) is used.</para>
    </note>
    <para>Some examples are shown below:</para>
    <itemizedlist>
      <listitem>
        <para> To obtain data from a Lustre client:</para>
        <screen># lctl list_param osc.*
osc.testfs-OST0000-osc-ffff881071d5cc00
osc.testfs-OST0001-osc-ffff881071d5cc00
osc.testfs-OST0002-osc-ffff881071d5cc00
osc.testfs-OST0003-osc-ffff881071d5cc00
osc.testfs-OST0004-osc-ffff881071d5cc00
osc.testfs-OST0005-osc-ffff881071d5cc00
osc.testfs-OST0006-osc-ffff881071d5cc00
osc.testfs-OST0007-osc-ffff881071d5cc00
osc.testfs-OST0008-osc-ffff881071d5cc00</screen>
        <para>In this example, information about OST connections available
	on a client is displayed (indicated by "osc").</para>
      </listitem>
    </itemizedlist>
    <itemizedlist>
      <listitem>
        <para> To see multiple levels of parameters, use multiple
          wildcards:<screen># lctl list_param osc.*.*
osc.testfs-OST0000-osc-ffff881071d5cc00.active
osc.testfs-OST0000-osc-ffff881071d5cc00.blocksize
osc.testfs-OST0000-osc-ffff881071d5cc00.checksum_type
osc.testfs-OST0000-osc-ffff881071d5cc00.checksums
osc.testfs-OST0000-osc-ffff881071d5cc00.connect_flags
osc.testfs-OST0000-osc-ffff881071d5cc00.contention_seconds
osc.testfs-OST0000-osc-ffff881071d5cc00.cur_dirty_bytes
...
osc.testfs-OST0000-osc-ffff881071d5cc00.rpc_stats</screen></para>
      </listitem>
    </itemizedlist>
    <itemizedlist>
      <listitem>
        <para> To view a specific file, use <literal>lctl get_param</literal>:
          <screen># lctl get_param osc.lustre-OST0000*.rpc_stats</screen></para>
      </listitem>
    </itemizedlist>
    <para>For more information about using <literal>lctl</literal>, see <xref
        xmlns:xlink="http://www.w3.org/1999/xlink" linkend="dbdoclet.50438194_51490"/>.</para>
    <para>Data can also be viewed using the <literal>cat</literal> command
    with the full path to the file. The form of the <literal>cat</literal>
    command is similar to that of the <literal>lctl get_param</literal>
    command with some differences.  Unfortunately, as the Linux kernel has
    changed over the years, the location of statistics and parameter files
    has also changed, which means that the Lustre parameter files may be
    located in either the <literal>/proc</literal> directory, in the
    <literal>/sys</literal> directory, and/or in the
    <literal>/sys/kernel/debug</literal> directory, depending on the kernel
    version and the Lustre version being used.  The <literal>lctl</literal>
    command insulates scripts from these changes and is preferred over direct
    file access, unless as part of a high-performance monitoring system.
    In the <literal>cat</literal> command:</para>
    <itemizedlist>
      <listitem>
        <para>Replace the dots in the path with slashes.</para>
      </listitem>
      <listitem>
        <para>Prepend the path with the appropriate directory component:
	  <screen>/{proc,sys}/{fs,sys}/{lustre,lnet}</screen></para>
      </listitem>
    </itemizedlist>
    <para>For example, an <literal>lctl get_param</literal> command may look like
      this:<screen># lctl get_param osc.*.uuid
osc.testfs-OST0000-osc-ffff881071d5cc00.uuid=594db456-0685-bd16-f59b-e72ee90e9819
osc.testfs-OST0001-osc-ffff881071d5cc00.uuid=594db456-0685-bd16-f59b-e72ee90e9819
...</screen></para>
    <para>The equivalent <literal>cat</literal> command may look like this:
     <screen># cat /proc/fs/lustre/osc/*/uuid
594db456-0685-bd16-f59b-e72ee90e9819
594db456-0685-bd16-f59b-e72ee90e9819
...</screen></para>
    <para>or like this:
     <screen># cat /sys/fs/lustre/osc/*/uuid
594db456-0685-bd16-f59b-e72ee90e9819
594db456-0685-bd16-f59b-e72ee90e9819
...</screen></para>
    <para>The <literal>llstat</literal> utility can be used to monitor some
    Lustre file system I/O activity over a specified time period. For more
    details, see
    <xref xmlns:xlink="http://www.w3.org/1999/xlink" linkend="dbdoclet.50438219_23232"/></para>
    <para>Some data is imported from attached clients and is available in a
    directory called <literal>exports</literal> located in the corresponding
    per-service directory on a Lustre server. For example:
    <screen>oss:/root# lctl list_param obdfilter.testfs-OST0000.exports.*
# hash ldlm_stats stats uuid</screen></para>
    <section remap="h3">
      <title>Identifying Lustre File Systems and Servers</title>
      <para>Several <literal>/proc</literal> files on the MGS list existing
      Lustre file systems and file system servers. The examples below are for
      a Lustre file system called
          <literal>testfs</literal> with one MDT and three OSTs.</para>
      <itemizedlist>
        <listitem>
          <para> To view all known Lustre file systems, enter:</para>
          <screen>mgs# lctl get_param mgs.*.filesystems
testfs</screen>
        </listitem>
        <listitem>
          <para> To view the names of the servers in a file system in which least one server is
            running,
            enter:<screen>lctl get_param mgs.*.live.<replaceable>&lt;filesystem name></replaceable></screen></para>
          <para>For example:</para>
          <screen>mgs# lctl get_param mgs.*.live.testfs
fsname: testfs
flags: 0x20     gen: 45
testfs-MDT0000
testfs-OST0000
testfs-OST0001
testfs-OST0002 

Secure RPC Config Rules: 

imperative_recovery_state:
    state: startup
    nonir_clients: 0
    nidtbl_version: 6
    notify_duration_total: 0.001000
    notify_duation_max:  0.001000
    notify_count: 4</screen>
        </listitem>
        <listitem>
          <para>To view the names of all live servers in the file system as listed in
              <literal>/proc/fs/lustre/devices</literal>, enter:</para>
          <screen># lctl device_list
0 UP mgs MGS MGS 11
1 UP mgc MGC192.168.10.34@tcp 1f45bb57-d9be-2ddb-c0b0-5431a49226705
2 UP mdt MDS MDS_uuid 3
3 UP lov testfs-mdtlov testfs-mdtlov_UUID 4
4 UP mds testfs-MDT0000 testfs-MDT0000_UUID 7
5 UP osc testfs-OST0000-osc testfs-mdtlov_UUID 5
6 UP osc testfs-OST0001-osc testfs-mdtlov_UUID 5
7 UP lov testfs-clilov-ce63ca00 08ac6584-6c4a-3536-2c6d-b36cf9cbdaa04
8 UP mdc testfs-MDT0000-mdc-ce63ca00 08ac6584-6c4a-3536-2c6d-b36cf9cbdaa05
9 UP osc testfs-OST0000-osc-ce63ca00 08ac6584-6c4a-3536-2c6d-b36cf9cbdaa05
10 UP osc testfs-OST0001-osc-ce63ca00 08ac6584-6c4a-3536-2c6d-b36cf9cbdaa05</screen>
          <para>The information provided on each line includes:</para>
          <para> -  Device number</para>
          <para> - Device status (UP, INactive, or STopping) </para>
          <para> -  Device name</para>
          <para> -  Device UUID</para>
          <para> -  Reference count (how many users this device has)</para>
        </listitem>
        <listitem>
          <para>To display the name of any server, view the device
            label:<screen>mds# e2label /dev/sda
testfs-MDT0000</screen></para>
        </listitem>
      </itemizedlist>
    </section>
  </section>
  <section>
    <title>Tuning Multi-Block Allocation (mballoc)</title>
    <para>Capabilities supported by <literal>mballoc</literal> include:</para>
    <itemizedlist>
      <listitem>
        <para> Pre-allocation for single files to help to reduce fragmentation.</para>
      </listitem>
      <listitem>
        <para> Pre-allocation for a group of files to enable packing of small files into large,
          contiguous chunks.</para>
      </listitem>
      <listitem>
        <para> Stream allocation to help decrease the seek rate.</para>
      </listitem>
    </itemizedlist>
    <para>The following <literal>mballoc</literal> tunables are available:</para>
    <informaltable frame="all">
      <tgroup cols="2">
        <colspec colname="c1" colwidth="30*"/>
        <colspec colname="c2" colwidth="70*"/>
        <thead>
          <row>
            <entry>
              <para><emphasis role="bold">Field</emphasis></para>
            </entry>
            <entry>
              <para><emphasis role="bold">Description</emphasis></para>
            </entry>
          </row>
        </thead>
        <tbody>
          <row>
            <entry>
              <para>
                <literal>mb_max_to_scan</literal></para>
            </entry>
            <entry>
              <para>Maximum number of free chunks that <literal>mballoc</literal> finds before a
                final decision to avoid a livelock situation.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>mb_min_to_scan</literal></para>
            </entry>
            <entry>
              <para>Minimum number of free chunks that <literal>mballoc</literal> searches before
                picking the best chunk for allocation. This is useful for small requests to reduce
                fragmentation of big free chunks.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>mb_order2_req</literal></para>
            </entry>
            <entry>
              <para>For requests equal to 2^N, where N &gt;= <literal>mb_order2_req</literal>, a
                fast search is done using a base 2 buddy allocation service.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>mb_small_req</literal></para>
            </entry>
            <entry morerows="1">
              <para><literal>mb_small_req</literal> - Defines (in MB) the upper bound of "small
                requests".</para>
              <para><literal>mb_large_req</literal> - Defines (in MB) the lower bound of "large
                requests".</para>
              <para>Requests are handled differently based on size:<itemizedlist>
                  <listitem>
                    <para>&lt; <literal>mb_small_req</literal> - Requests are packed together to
                      form large, aggregated requests.</para>
                  </listitem>
                  <listitem>
                    <para>> <literal>mb_small_req</literal> and &lt; <literal>mb_large_req</literal>
                      - Requests are primarily allocated linearly.</para>
                  </listitem>
                  <listitem>
                    <para>> <literal>mb_large_req</literal> - Requests are allocated since hard disk
                      seek time is less of a concern in this case.</para>
                  </listitem>
                </itemizedlist></para>
              <para>In general, small requests are combined to create larger requests, which are
                then placed close to one another to minimize the number of seeks required to access
                the data.</para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>mb_large_req</literal></para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>mb_prealloc_table</literal></para>
            </entry>
            <entry>
              <para>A table of values used to preallocate space when a new request is received. By
                default, the table looks like
                this:<screen>prealloc_table
4 8 16 32 64 128 256 512 1024 2048 </screen></para>
              <para>When a new request is received, space is preallocated at the next higher
                increment specified in the table. For example, for requests of less than 4 file
                system blocks, 4 blocks of space are preallocated; for requests between 4 and 8, 8
                blocks are preallocated; and so forth</para>
              <para>Although customized values can be entered in the table, the performance of
                general usage file systems will not typically be improved by modifying the table (in
                fact, in ext4 systems, the table values are fixed).  However, for some specialized
                workloads, tuning the <literal>prealloc_table</literal> values may result in smarter
                preallocation decisions. </para>
            </entry>
          </row>
          <row>
            <entry>
              <para>
                <literal>mb_group_prealloc</literal></para>
            </entry>
            <entry>
              <para>The amount of space (in kilobytes) preallocated for groups of small
                requests.</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <para>Buddy group cache information found in
          <literal>/proc/fs/ldiskfs/<replaceable>disk_device</replaceable>/mb_groups</literal> may
      be useful for assessing on-disk fragmentation. For
      example:<screen>cat /proc/fs/ldiskfs/loop0/mb_groups 
#group: free free frags first pa [ 2^0 2^1 2^2 2^3 2^4 2^5 2^6 2^7 2^8 2^9 
     2^10 2^11 2^12 2^13] 
#0    : 2936 2936 1     42    0  [ 0   0   0   1   1   1   1   2   0   1 
     2    0    0    0   ]</screen></para>
    <para>In this example, the columns show:<itemizedlist>
        <listitem>
          <para>#group number</para>
        </listitem>
        <listitem>
          <para>Available blocks in the group</para>
        </listitem>
        <listitem>
          <para>Blocks free on a disk</para>
        </listitem>
        <listitem>
          <para>Number of free fragments</para>
        </listitem>
        <listitem>
          <para>First free block in the group</para>
        </listitem>
        <listitem>
          <para>Number of preallocated chunks (not blocks)</para>
        </listitem>
        <listitem>
          <para>A series of available chunks of different sizes</para>
        </listitem>
      </itemizedlist></para>
  </section>
  <section>
    <title>Monitoring Lustre File System I/O</title>
    <para>A number of system utilities are provided to enable collection of data related to I/O
      activity in a Lustre file system. In general, the data collected describes:</para>
    <itemizedlist>
      <listitem>
        <para> Data transfer rates and throughput of inputs and outputs external to the Lustre file
          system, such as network requests or disk I/O operations performed</para>
      </listitem>
      <listitem>
        <para> Data about the throughput or transfer rates of internal Lustre file system data, such
          as locks or allocations. </para>
      </listitem>
    </itemizedlist>
    <note>
      <para>It is highly recommended that you complete baseline testing for your Lustre file system
        to determine normal I/O activity for your hardware, network, and system workloads. Baseline
        data will allow you to easily determine when performance becomes degraded in your system.
        Two particularly useful baseline statistics are:</para>
      <itemizedlist>
        <listitem>
          <para><literal>brw_stats</literal> – Histogram data characterizing I/O requests to the
            OSTs. For more details, see <xref xmlns:xlink="http://www.w3.org/1999/xlink"
              linkend="dbdoclet.50438271_55057"/>.</para>
        </listitem>
        <listitem>
          <para><literal>rpc_stats</literal> – Histogram data showing information about RPCs made by
            clients. For more details, see <xref xmlns:xlink="http://www.w3.org/1999/xlink"
              linkend="MonitoringClientRCPStream"/>.</para>
        </listitem>
      </itemizedlist>
    </note>
    <section remap="h3" xml:id="MonitoringClientRCPStream">
      <title><indexterm>
          <primary>proc</primary>
          <secondary>watching RPC</secondary>
        </indexterm>Monitoring the Client RPC Stream</title>
      <para>The <literal>rpc_stats</literal> file contains histogram data showing information about
        remote procedure calls (RPCs) that have been made since this file was last cleared. The
        histogram data can be cleared by writing any value into the <literal>rpc_stats</literal>
        file.</para>
      <para><emphasis role="italic"><emphasis role="bold">Example:</emphasis></emphasis></para>
      <screen># lctl get_param osc.testfs-OST0000-osc-ffff810058d2f800.rpc_stats
snapshot_time:            1372786692.389858 (secs.usecs)
read RPCs in flight:      0
write RPCs in flight:     1
dio read RPCs in flight:  0
dio write RPCs in flight: 0
pending write pages:      256
pending read pages:       0

                     read                   write
pages per rpc   rpcs   % cum % |       rpcs   % cum %
1:                 0   0   0   |          0   0   0
2:                 0   0   0   |          1   0   0
4:                 0   0   0   |          0   0   0
8:                 0   0   0   |          0   0   0
16:                0   0   0   |          0   0   0
32:                0   0   0   |          2   0   0
64:                0   0   0   |          2   0   0
128:               0   0   0   |          5   0   0
256:             850 100 100   |      18346  99 100

                     read                   write
rpcs in flight  rpcs   % cum % |       rpcs   % cum %
0:               691  81  81   |       1740   9   9
1:                48   5  86   |        938   5  14
2:                29   3  90   |       1059   5  20
3:                17   2  92   |       1052   5  26
4:                13   1  93   |        920   5  31
5:                12   1  95   |        425   2  33
6:                10   1  96   |        389   2  35
7:                30   3 100   |      11373  61  97
8:                 0   0 100   |        460   2 100

                     read                   write
offset          rpcs   % cum % |       rpcs   % cum %
0:               850 100 100   |      18347  99  99
1:                 0   0 100   |          0   0  99
2:                 0   0 100   |          0   0  99
4:                 0   0 100   |          0   0  99
8:                 0   0 100   |          0   0  99
16:                0   0 100   |          1   0  99
32:                0   0 100   |          1   0  99
64:                0   0 100   |          3   0  99
128:               0   0 100   |          4   0 100

</screen>
      <para>The header information includes:</para>
      <itemizedlist>
        <listitem>
          <para><literal>snapshot_time</literal> - UNIX epoch instant the file was read.</para>
        </listitem>
        <listitem>
          <para><literal>read RPCs in flight</literal> - Number of read RPCs issued by the OSC, but
            not complete at the time of the snapshot. This value should always be less than or equal
            to <literal>max_rpcs_in_flight</literal>.</para>
        </listitem>
        <listitem>
          <para><literal>write RPCs in flight</literal> - Number of write RPCs issued by the OSC,
            but not complete at the time of the snapshot. This value should always be less than or
            equal to <literal>max_rpcs_in_flight</literal>.</para>
        </listitem>
        <listitem>
          <para><literal>dio read RPCs in flight</literal> - Direct I/O (as opposed to block I/O)
            read RPCs issued but not completed at the time of the snapshot.</para>
        </listitem>
        <listitem>
          <para><literal>dio write RPCs in flight</literal> - Direct I/O (as opposed to block I/O)
            write RPCs issued but not completed at the time of the snapshot.</para>
        </listitem>
        <listitem>
          <para><literal>pending write pages</literal>  - Number of pending write pages that have
            been queued for I/O in the OSC.</para>
        </listitem>
        <listitem>
          <para><literal>pending read pages</literal> - Number of pending read pages that have been
            queued for I/O in the OSC.</para>
        </listitem>
      </itemizedlist>
      <para>The tabular data is described in the table below. Each row in the table shows the number
        of reads or writes (<literal>ios</literal>) occurring for the statistic, the relative
        percentage (<literal>%</literal>) of total reads or writes, and the cumulative percentage
          (<literal>cum %</literal>) to that point in the table for the statistic.</para>
      <informaltable frame="all">
        <tgroup cols="2">
          <colspec colname="c1" colwidth="40*"/>
          <colspec colname="c2" colwidth="60*"/>
          <thead>
            <row>
              <entry>
                <para><emphasis role="bold">Field</emphasis></para>
              </entry>
              <entry>
                <para><emphasis role="bold">Description</emphasis></para>
              </entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>
                <para> pages per RPC</para>
              </entry>
              <entry>
                <para>Shows cumulative RPC reads and writes organized according to the number of
                  pages in the RPC. A single page RPC increments the <literal>0:</literal>
                  row.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para> RPCs in flight</para>
              </entry>
              <entry>
                <para> Shows the number of RPCs that are pending when an RPC is sent. When the first
                  RPC is sent, the <literal>0:</literal> row is incremented. If the first RPC is
                  sent while another RPC is pending, the <literal>1:</literal> row is incremented
                  and so on. </para>
              </entry>
            </row>
            <row>
              <entry>
                <para> offset</para>
              </entry>
              <entry>
                <para> The page index of the first page read from or written to the object by the
                  RPC. </para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>
      <para><emphasis role="italic"><emphasis role="bold">Analysis:</emphasis></emphasis></para>
      <para>This table provides a way to visualize the concurrency of the RPC stream. Ideally, you
        will see a large clump around the <literal>max_rpcs_in_flight value</literal>, which shows
        that the network is being kept busy.</para>
      <para>For information about optimizing the client I/O RPC stream, see <xref
          xmlns:xlink="http://www.w3.org/1999/xlink" linkend="TuningClientIORPCStream"/>.</para>
    </section>
    <section xml:id="lustreproc.clientstats" remap="h3">
      <title><indexterm>
          <primary>proc</primary>
          <secondary>client stats</secondary>
        </indexterm>Monitoring Client Activity</title>
      <para>The <literal>stats</literal> file maintains statistics accumulate during typical
        operation of a client across the VFS interface of the Lustre file system. Only non-zero
        parameters are displayed in the file. </para>
      <para>Client statistics are enabled by default.</para>
      <note>
        <para>Statistics for all mounted file systems can be discovered by
          entering:<screen>lctl get_param llite.*.stats</screen></para>
      </note>
      <para><emphasis role="italic"><emphasis role="bold">Example:</emphasis></emphasis></para>
      <screen>client# lctl get_param llite.*.stats
snapshot_time          1308343279.169704 secs.usecs
dirty_pages_hits       14819716 samples [regs]
dirty_pages_misses     81473472 samples [regs]
read_bytes             36502963 samples [bytes] 1 26843582 55488794
write_bytes            22985001 samples [bytes] 0 125912 3379002
brw_read               2279 samples [pages] 1 1 2270
ioctl                  186749 samples [regs]
open                   3304805 samples [regs]
close                  3331323 samples [regs]
seek                   48222475 samples [regs]
fsync                  963 samples [regs]
truncate               9073 samples [regs]
setxattr               19059 samples [regs]
getxattr               61169 samples [regs]
</screen>
      <para> The statistics can be cleared by echoing an empty string into the
          <literal>stats</literal> file or by using the command:
        <screen>lctl set_param llite.*.stats=0</screen></para>
      <para>The statistics displayed are described in the table below.</para>
      <informaltable frame="all">
        <tgroup cols="2">
          <colspec colname="c1" colwidth="3*"/>
          <colspec colname="c2" colwidth="7*"/>
          <thead>
            <row>
              <entry>
                <para><emphasis role="bold">Entry</emphasis></para>
              </entry>
              <entry>
                <para><emphasis role="bold">Description</emphasis></para>
              </entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>
                <para>
                  <literal>snapshot_time</literal></para>
              </entry>
              <entry>
                <para>UNIX epoch instant the stats file was read.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal>dirty_page_hits</literal></para>
              </entry>
              <entry>
                <para>The number of write operations that have been satisfied by the dirty page
                  cache. See <xref xmlns:xlink="http://www.w3.org/1999/xlink"
                    linkend="TuningClientIORPCStream"/> for more information about dirty cache
                  behavior in a Lustre file system.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal>dirty_page_misses</literal></para>
              </entry>
              <entry>
                <para>The number of write operations that were not satisfied by the dirty page
                  cache.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal>read_bytes</literal></para>
              </entry>
              <entry>
                <para>The number of read operations that have occurred. Three additional parameters
                  are displayed:</para>
                <variablelist>
                  <varlistentry>
                    <term>min</term>
                    <listitem>
                      <para>The minimum number of bytes read in a single request since the counter
                        was reset.</para>
                    </listitem>
                  </varlistentry>
                  <varlistentry>
                    <term>max</term>
                    <listitem>
                      <para>The maximum number of bytes read in a single request since the counter
                        was reset.</para>
                    </listitem>
                  </varlistentry>
                  <varlistentry>
                    <term>sum</term>
                    <listitem>
                      <para>The accumulated sum of bytes of all read requests since the counter was
                        reset.</para>
                    </listitem>
                  </varlistentry>
                </variablelist>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal>write_bytes</literal></para>
              </entry>
              <entry>
                <para>The number of write operations that have occurred. Three additional parameters
                  are displayed:</para>
                <variablelist>
                  <varlistentry>
                    <term>min</term>
                    <listitem>
                      <para>The minimum number of bytes written in a single request since the
                        counter was reset.</para>
                    </listitem>
                  </varlistentry>
                  <varlistentry>
                    <term>max</term>
                    <listitem>
                      <para>The maximum number of bytes written in a single request since the
                        counter was reset.</para>
                    </listitem>
                  </varlistentry>
                  <varlistentry>
                    <term>sum</term>
                    <listitem>
                      <para>The accumulated sum of bytes of all write requests since the counter was
                        reset.</para>
                    </listitem>
                  </varlistentry>
                </variablelist>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal>brw_read</literal></para>
              </entry>
              <entry>
                <para>The number of pages that have been read. Three additional parameters are
                  displayed:</para>
                <variablelist>
                  <varlistentry>
                    <term>min</term>
                    <listitem>
                      <para>The minimum number of bytes read in a single block read/write
                          (<literal>brw</literal>) read request since the counter was reset.</para>
                    </listitem>
                  </varlistentry>
                  <varlistentry>
                    <term>max</term>
                    <listitem>
                      <para>The maximum number of bytes read in a single <literal>brw</literal> read
                        requests since the counter was reset.</para>
                    </listitem>
                  </varlistentry>
                  <varlistentry>
                    <term>sum</term>
                    <listitem>
                      <para>The accumulated sum of bytes of all <literal>brw</literal> read requests
                        since the counter was reset.</para>
                    </listitem>
                  </varlistentry>
                </variablelist>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal>ioctl</literal></para>
              </entry>
              <entry>
                <para>The number of combined file and directory <literal>ioctl</literal>
                  operations.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal>open</literal></para>
              </entry>
              <entry>
                <para>The number of open operations that have succeeded.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal>close</literal></para>
              </entry>
              <entry>
                <para>The number of close operations that have succeeded.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal>seek</literal></para>
              </entry>
              <entry>
                <para>The number of times <literal>seek</literal> has been called.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal>fsync</literal></para>
              </entry>
              <entry>
                <para>The number of times <literal>fsync</literal> has been called.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal>truncate</literal></para>
              </entry>
              <entry>
                <para>The total number of calls to both locked and lockless
                    <literal>truncate</literal>.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal>setxattr</literal></para>
              </entry>
              <entry>
                <para>The number of times extended attributes have been set. </para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal>getxattr</literal></para>
              </entry>
              <entry>
                <para>The number of times value(s) of extended attributes have been fetched.</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>
      <para><emphasis role="italic"><emphasis role="bold">Analysis:</emphasis></emphasis></para>
      <para>Information is provided about the amount and type of I/O activity is taking place on the
        client.</para>
    </section>
    <section remap="h3">
      <title><indexterm>
          <primary>proc</primary>
          <secondary>read/write survey</secondary>
        </indexterm>Monitoring Client Read-Write Offset Statistics</title>
      <para>When the <literal>offset_stats</literal> parameter is set, statistics are maintained for
        occurrences of a series of read or write calls from a process that did not access the next
        sequential location. The <literal>OFFSET</literal> field is reset to 0 (zero) whenever a
        different file is read or written.</para>
      <note>
        <para>By default, statistics are not collected in the <literal>offset_stats</literal>,
            <literal>extents_stats</literal>, and <literal>extents_stats_per_process</literal> files
          to reduce monitoring overhead when this information is not needed.  The collection of
          statistics in all three of these files is activated by writing
          anything, except for 0 (zero) and "disable", into any one of the
          files.</para>
      </note>
      <para><emphasis role="italic"><emphasis role="bold">Example:</emphasis></emphasis></para>
      <screen># lctl get_param llite.testfs-f57dee0.offset_stats
snapshot_time: 1155748884.591028 (secs.usecs)
             RANGE   RANGE    SMALLEST   LARGEST
R/W   PID    START   END      EXTENT     EXTENT    OFFSET
R     8385   0       128      128        128       0
R     8385   0       224      224        224       -128
W     8385   0       250      50         100       0
W     8385   100     1110     10         500       -150
W     8384   0       5233     5233       5233      0
R     8385   500     600      100        100       -610</screen>
      <para>In this example, <literal>snapshot_time</literal> is the UNIX epoch instant the file was
        read. The tabular data is described in the table below.</para>
      <para>The <literal>offset_stats</literal> file can be cleared by
        entering:<screen>lctl set_param llite.*.offset_stats=0</screen></para>
      <informaltable frame="all">
        <tgroup cols="2">
          <colspec colname="c1" colwidth="50*"/>
          <colspec colname="c2" colwidth="50*"/>
          <thead>
            <row>
              <entry>
                <para><emphasis role="bold">Field</emphasis></para>
              </entry>
              <entry>
                <para><emphasis role="bold">Description</emphasis></para>
              </entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>
                <para>R/W</para>
              </entry>
              <entry>
                <para>Indicates if the non-sequential call was a read or write</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>PID </para>
              </entry>
              <entry>
                <para>Process ID of the process that made the read/write call.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>RANGE START/RANGE END</para>
              </entry>
              <entry>
                <para>Range in which the read/write calls were sequential.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>SMALLEST EXTENT </para>
              </entry>
              <entry>
                <para>Smallest single read/write in the corresponding range (in bytes).</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>LARGEST EXTENT </para>
              </entry>
              <entry>
                <para>Largest single read/write in the corresponding range (in bytes).</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>OFFSET </para>
              </entry>
              <entry>
                <para>Difference between the previous range end and the current range start.</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>
      <para><emphasis role="italic"><emphasis role="bold">Analysis:</emphasis></emphasis></para>
      <para>This data provides an indication of how contiguous or fragmented the data is. For
        example, the fourth entry in the example above shows the writes for this RPC were sequential
        in the range 100 to 1110 with the minimum write 10 bytes and the maximum write 500 bytes.
        The range started with an offset of -150 from the <literal>RANGE END</literal> of the
        previous entry in the example.</para>
    </section>
    <section remap="h3">
      <title><indexterm>
          <primary>proc</primary>
          <secondary>read/write survey</secondary>
        </indexterm>Monitoring Client Read-Write Extent Statistics</title>
      <para>For in-depth troubleshooting, client read-write extent statistics can be accessed to
        obtain more detail about read/write I/O extents for the file system or for a particular
        process.</para>
      <note>
        <para>By default, statistics are not collected in the <literal>offset_stats</literal>,
            <literal>extents_stats</literal>, and <literal>extents_stats_per_process</literal> files
          to reduce monitoring overhead when this information is not needed.  The collection of
          statistics in all three of these files is activated by writing
          anything, except for 0 (zero) and "disable", into any one of the
          files.</para>
      </note>
      <section remap="h3">
        <title>Client-Based I/O Extent Size Survey</title>
        <para>The <literal>extents_stats</literal> histogram in the
          <literal>llite</literal> directory shows the statistics for the sizes
          of the read/write I/O extents. This file does not maintain the per
          process statistics.</para>
        <para><emphasis role="italic"><emphasis role="bold">Example:</emphasis></emphasis></para>
        <screen># lctl get_param llite.testfs-*.extents_stats
snapshot_time:                     1213828728.348516 (secs.usecs)
                       read           |            write
extents          calls  %      cum%   |     calls  %     cum%

0K - 4K :        0      0      0      |     2      2     2
4K - 8K :        0      0      0      |     0      0     2
8K - 16K :       0      0      0      |     0      0     2
16K - 32K :      0      0      0      |     20     23    26
32K - 64K :      0      0      0      |     0      0     26
64K - 128K :     0      0      0      |     51     60    86
128K - 256K :    0      0      0      |     0      0     86
256K - 512K :    0      0      0      |     0      0     86
512K - 1024K :   0      0      0      |     0      0     86
1M - 2M :        0      0      0      |     11     13    100</screen>
        <para>In this example, <literal>snapshot_time</literal> is the UNIX epoch instant the file
          was read. The table shows cumulative extents organized according to size with statistics
          provided separately for reads and writes. Each row in the table shows the number of RPCs
          for reads and writes respectively (<literal>calls</literal>), the relative percentage of
          total calls (<literal>%</literal>), and the cumulative percentage to
          that point in the table of calls (<literal>cum %</literal>). </para>
        <para> The file can be cleared by issuing the following command:
        <screen># lctl set_param llite.testfs-*.extents_stats=1</screen></para>
      </section>
      <section>
        <title>Per-Process Client I/O Statistics</title>
        <para>The <literal>extents_stats_per_process</literal> file maintains the I/O extent size
          statistics on a per-process basis.</para>
        <para><emphasis role="italic"><emphasis role="bold">Example:</emphasis></emphasis></para>
        <screen># lctl get_param llite.testfs-*.extents_stats_per_process
snapshot_time:                     1213828762.204440 (secs.usecs)
                          read            |             write
extents            calls   %      cum%    |      calls   %       cum%
 
PID: 11488
   0K - 4K :       0       0       0      |      0       0       0
   4K - 8K :       0       0       0      |      0       0       0
   8K - 16K :      0       0       0      |      0       0       0
   16K - 32K :     0       0       0      |      0       0       0
   32K - 64K :     0       0       0      |      0       0       0
   64K - 128K :    0       0       0      |      0       0       0
   128K - 256K :   0       0       0      |      0       0       0
   256K - 512K :   0       0       0      |      0       0       0
   512K - 1024K :  0       0       0      |      0       0       0
   1M - 2M :       0       0       0      |      10      100     100
 
PID: 11491
   0K - 4K :       0       0       0      |      0       0       0
   4K - 8K :       0       0       0      |      0       0       0
   8K - 16K :      0       0       0      |      0       0       0
   16K - 32K :     0       0       0      |      20      100     100
   
PID: 11424
   0K - 4K :       0       0       0      |      0       0       0
   4K - 8K :       0       0       0      |      0       0       0
   8K - 16K :      0       0       0      |      0       0       0
   16K - 32K :     0       0       0      |      0       0       0
   32K - 64K :     0       0       0      |      0       0       0
   64K - 128K :    0       0       0      |      16      100     100
 
PID: 11426
   0K - 4K :       0       0       0      |      1       100     100
 
PID: 11429
   0K - 4K :       0       0       0      |      1       100     100
 
</screen>
        <para>This table shows cumulative extents organized according to size for each process ID
          (PID) with statistics provided separately for reads and writes. Each row in the table
          shows the number of RPCs for reads and writes respectively (<literal>calls</literal>), the
          relative percentage of total calls (<literal>%</literal>), and the cumulative percentage
          to that point in the table of calls (<literal>cum %</literal>). </para>
      </section>
    </section>
    <section xml:id="dbdoclet.50438271_55057">
      <title><indexterm>
          <primary>proc</primary>
          <secondary>block I/O</secondary>
        </indexterm>Monitoring the OST Block I/O Stream</title>
      <para>The <literal>brw_stats</literal> file in the <literal>obdfilter</literal> directory
        contains histogram data showing statistics for number of I/O requests sent to the disk,
        their size, and whether they are contiguous on the disk or not.</para>
      <para><emphasis role="italic"><emphasis role="bold">Example:</emphasis></emphasis></para>
      <para>Enter on the OSS:</para>
      <screen># lctl get_param obdfilter.testfs-OST0000.brw_stats 
snapshot_time:         1372775039.769045 (secs.usecs)
                           read      |      write
pages per bulk r/w     rpcs  % cum % |  rpcs   % cum %
1:                     108 100 100   |    39   0   0
2:                       0   0 100   |     6   0   0
4:                       0   0 100   |     1   0   0
8:                       0   0 100   |     0   0   0
16:                      0   0 100   |     4   0   0
32:                      0   0 100   |    17   0   0
64:                      0   0 100   |    12   0   0
128:                     0   0 100   |    24   0   0
256:                     0   0 100   | 23142  99 100

                           read      |      write
discontiguous pages    rpcs  % cum % |  rpcs   % cum %
0:                     108 100 100   | 23245 100 100

                           read      |      write
discontiguous blocks   rpcs  % cum % |  rpcs   % cum %
0:                     108 100 100   | 23243  99  99
1:                       0   0 100   |     2   0 100

                           read      |      write
disk fragmented I/Os   ios   % cum % |   ios   % cum %
0:                      94  87  87   |     0   0   0
1:                      14  12 100   | 23243  99  99
2:                       0   0 100   |     2   0 100

                           read      |      write
disk I/Os in flight    ios   % cum % |   ios   % cum %
1:                      14 100 100   | 20896  89  89
2:                       0   0 100   |  1071   4  94
3:                       0   0 100   |   573   2  96
4:                       0   0 100   |   300   1  98
5:                       0   0 100   |   166   0  98
6:                       0   0 100   |   108   0  99
7:                       0   0 100   |    81   0  99
8:                       0   0 100   |    47   0  99
9:                       0   0 100   |     5   0 100

                           read      |      write
I/O time (1/1000s)     ios   % cum % |   ios   % cum %
1:                      94  87  87   |     0   0   0
2:                       0   0  87   |     7   0   0
4:                      14  12 100   |    27   0   0
8:                       0   0 100   |    14   0   0
16:                      0   0 100   |    31   0   0
32:                      0   0 100   |    38   0   0
64:                      0   0 100   | 18979  81  82
128:                     0   0 100   |   943   4  86
256:                     0   0 100   |  1233   5  91
512:                     0   0 100   |  1825   7  99
1K:                      0   0 100   |   99   0  99
2K:                      0   0 100   |     0   0  99
4K:                      0   0 100   |     0   0  99
8K:                      0   0 100   |    49   0 100

                           read      |      write
disk I/O size          ios   % cum % |   ios   % cum %
4K:                     14 100 100   |    41   0   0
8K:                      0   0 100   |     6   0   0
16K:                     0   0 100   |     1   0   0
32K:                     0   0 100   |     0   0   0
64K:                     0   0 100   |     4   0   0
128K:                    0   0 100   |    17   0   0
256K:                    0   0 100   |    12   0   0
512K:                    0   0 100   |    24   0   0
1M:                      0   0 100   | 23142  99 100
</screen>
      <para>The tabular data is described in the table below. Each row in the table shows the number
        of reads and writes occurring for the statistic (<literal>ios</literal>), the relative
        percentage of total reads or writes (<literal>%</literal>), and the cumulative percentage to
        that point in the table for the statistic (<literal>cum %</literal>). </para>
      <informaltable frame="all">
        <tgroup cols="2">
          <colspec colname="c1" colwidth="40*"/>
          <colspec colname="c2" colwidth="60*"/>
          <thead>
            <row>
              <entry>
                <para><emphasis role="bold">Field</emphasis></para>
              </entry>
              <entry>
                <para><emphasis role="bold">Description</emphasis></para>
              </entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>
                <para>
                  <literal>pages per bulk r/w</literal></para>
              </entry>
              <entry>
                <para>Number of pages per RPC request, which should match aggregate client
                    <literal>rpc_stats</literal> (see <xref
                    xmlns:xlink="http://www.w3.org/1999/xlink" linkend="MonitoringClientRCPStream"
                  />).</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal>discontiguous pages</literal></para>
              </entry>
              <entry>
                <para>Number of discontinuities in the logical file offset of each page in a single
                  RPC.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal>discontiguous blocks</literal></para>
              </entry>
              <entry>
                <para>Number of discontinuities in the physical block allocation in the file system
                  for a single RPC.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para><literal>disk fragmented I/Os</literal></para>
              </entry>
              <entry>
                <para>Number of I/Os that were not written entirely sequentially.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para><literal>disk I/Os in flight</literal></para>
              </entry>
              <entry>
                <para>Number of disk I/Os currently pending.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para><literal>I/O time (1/1000s)</literal></para>
              </entry>
              <entry>
                <para>Amount of time for each I/O operation to complete.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para><literal>disk I/O size</literal></para>
              </entry>
              <entry>
                <para>Size of each I/O operation.</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>
      <para><emphasis role="italic"><emphasis role="bold">Analysis:</emphasis></emphasis></para>
      <para>This data provides an indication of extent size and distribution in the file
        system.</para>
    </section>
  </section>
  <section>
    <title>Tuning Lustre File System I/O</title>
    <para>Each OSC has its own tree of tunables. For example:</para>
    <screen>$ lctl lctl list_param osc.*.*
osc.myth-OST0000-osc-ffff8804296c2800.active
osc.myth-OST0000-osc-ffff8804296c2800.blocksize
osc.myth-OST0000-osc-ffff8804296c2800.checksum_dump
osc.myth-OST0000-osc-ffff8804296c2800.checksum_type
osc.myth-OST0000-osc-ffff8804296c2800.checksums
osc.myth-OST0000-osc-ffff8804296c2800.connect_flags
:
:
osc.myth-OST0000-osc-ffff8804296c2800.state
osc.myth-OST0000-osc-ffff8804296c2800.stats
osc.myth-OST0000-osc-ffff8804296c2800.timeouts
osc.myth-OST0000-osc-ffff8804296c2800.unstable_stats
osc.myth-OST0000-osc-ffff8804296c2800.uuid
osc.myth-OST0001-osc-ffff8804296c2800.active
osc.myth-OST0001-osc-ffff8804296c2800.blocksize
osc.myth-OST0001-osc-ffff8804296c2800.checksum_dump
osc.myth-OST0001-osc-ffff8804296c2800.checksum_type
:
:
</screen>
    <para>The following sections describe some of the parameters that can
      be tuned in a Lustre file system.</para>
    <section remap="h3" xml:id="TuningClientIORPCStream">
      <title><indexterm>
          <primary>proc</primary>
          <secondary>RPC tunables</secondary>
        </indexterm>Tuning the Client I/O RPC Stream</title>
      <para>Ideally, an optimal amount of data is packed into each I/O RPC
        and a consistent number of issued RPCs are in progress at any time.
        To help optimize the client I/O RPC stream, several tuning variables
        are provided to adjust behavior according to network conditions and
        cluster size. For information about monitoring the client I/O RPC
        stream, see <xref
          xmlns:xlink="http://www.w3.org/1999/xlink" linkend="MonitoringClientRCPStream"/>.</para>
      <para>RPC stream tunables include:</para>
      <para>
        <itemizedlist>
          <listitem>
            <para><literal>osc.<replaceable>osc_instance</replaceable>.checksums</literal>
              - Controls whether the client will calculate data integrity
              checksums for the bulk data transferred to the OST.  Data
              integrity checksums are enabled by default.  The algorithm used
              can be set using the <literal>checksum_type</literal> parameter.
            </para>
          </listitem>
          <listitem>
            <para><literal>osc.<replaceable>osc_instance</replaceable>.checksum_type</literal>
              - Controls the data integrity checksum algorithm used by the
              client.  The available algorithms are determined by the set of
              algorihtms.  The checksum algorithm used by default is determined
              by first selecting the fastest algorithms available on the OST,
              and then selecting the fastest of those algorithms on the client,
              which depends on available optimizations in the CPU hardware and
              kernel.  The default algorithm can be overridden by writing the
              algorithm name into the <literal>checksum_type</literal>
              parameter.  Available checksum types can be seen on the client by
              reading the <literal>checksum_type</literal> parameter. Currently
              supported checksum types are:
              <literal>adler</literal>,
              <literal>crc32</literal>,
              <literal>crc32c</literal>
            </para>
            <para condition="l2C">
              In Lustre release 2.12 additional checksum types were added to
              allow end-to-end checksum integration with T10-PI capable
              hardware.  The client will compute the appropriate checksum
              type, based on the checksum type used by the storage, for the
              RPC checksum, which will be verified by the server and passed
              on to the storage.  The T10-PI checksum types are:
              <literal>t10ip512</literal>,
              <literal>t10ip4K</literal>,
              <literal>t10crc512</literal>,
              <literal>t10crc4K</literal>
            </para>
          </listitem>
          <listitem>
            <para><literal>osc.<replaceable>osc_instance</replaceable>.max_dirty_mb</literal>
              - Controls how many MiB of dirty data can be written into the
              client pagecache for writes by <emphasis>each</emphasis> OSC.
              When this limit is reached, additional writes block until
              previously-cached data is written to the server. This may be
              changed by the <literal>lctl set_param</literal> command. Only
              values larger than 0 and smaller than the lesser of 2048 MiB or
              1/4 of client RAM are valid. Performance can suffers if the
              client cannot aggregate enough data per OSC to form a full RPC
              (as set by the <literal>max_pages_per_rpc</literal>) parameter,
              unless the application is doing very large writes itself.
            </para>
            <para>To maximize performance, the value for
              <literal>max_dirty_mb</literal> is recommended to be at least
              4 * <literal>max_pages_per_rpc</literal> *
              <literal>max_rpcs_in_flight</literal>.
            </para>
          </listitem>
          <listitem>
            <para><literal>osc.<replaceable>osc_instance</replaceable>.cur_dirty_bytes</literal>
              - A read-only value that returns the current number of bytes
              written and cached by this OSC.
            </para>
          </listitem>
          <listitem>
            <para><literal>osc.<replaceable>osc_instance</replaceable>.max_pages_per_rpc</literal>
              - The maximum number of pages that will be sent in a single RPC
              request to the OST. The minimum value is one page and the maximum
              value is 16 MiB (4096 on systems with <literal>PAGE_SIZE</literal>
              of 4 KiB), with the default value of 4 MiB in one RPC.  The upper
              limit may also be constrained by <literal>ofd.*.brw_size</literal>
              setting on the OSS, and applies to all clients connected to that
              OST.  It is also possible to specify a units suffix (e.g.
              <literal>max_pages_per_rpc=4M</literal>), so the RPC size can be
              set independently of the client <literal>PAGE_SIZE</literal>.
            </para>
          </listitem>
          <listitem>
            <para><literal>osc.<replaceable>osc_instance</replaceable>.max_rpcs_in_flight</literal>
              - The maximum number of concurrent RPCs in flight from an OSC to
              its OST. If the OSC tries to initiate an RPC but finds that it
              already has the same number of RPCs outstanding, it will wait to
              issue further RPCs until some complete. The minimum setting is 1
              and maximum setting is 256. The default value is 8 RPCs.
            </para>
            <para>To improve small file I/O performance, increase the
              <literal>max_rpcs_in_flight</literal> value.
            </para>
          </listitem>
          <listitem>
            <para><literal>llite.<replaceable>fsname_instance</replaceable>.max_cache_mb</literal>
              - Maximum amount of inactive data cached by the client.  The
              default value is 3/4 of the client RAM.
            </para>
          </listitem>
        </itemizedlist>
      </para>
      <note>
        <para>The value for <literal><replaceable>osc_instance</replaceable></literal>
          and <literal><replaceable>fsname_instance</replaceable></literal>
          are unique to each mount point to allow associating osc, mdc, lov,
          lmv, and llite parameters with the same mount point.  However, it is
          common for scripts to use a wildcard <literal>*</literal> or a
          filesystem-specific wildcard
          <literal><replaceable>fsname-*</replaceable></literal> to specify
          the parameter settings uniformly on all clients. For example:
<screen>
client$ lctl get_param osc.testfs-OST0000*.rpc_stats
osc.testfs-OST0000-osc-ffff88107412f400.rpc_stats=
snapshot_time:         1375743284.337839 (secs.usecs)
read RPCs in flight:  0
write RPCs in flight: 0
</screen></para>
      </note>
    </section>
    <section remap="h3" xml:id="TuningClientReadahead">
      <title><indexterm>
          <primary>proc</primary>
          <secondary>readahead</secondary>
        </indexterm>Tuning File Readahead and Directory Statahead</title>
      <para>File readahead and directory statahead enable reading of data
      into memory before a process requests the data. File readahead prefetches
      file content data into memory for <literal>read()</literal> related
      calls, while directory statahead fetches file metadata into memory for
      <literal>readdir()</literal> and <literal>stat()</literal> related
      calls.  When readahead and statahead work well, a process that accesses
      data finds that the information it needs is available immediately in
      memory on the client when requested without the delay of network I/O.
      </para>
      <section remap="h4">
        <title>Tuning File Readahead</title>
        <para>File readahead is triggered when two or more sequential reads
          by an application fail to be satisfied by data in the Linux buffer
          cache. The size of the initial readahead is determined by the RPC
          size and the file stripe size, but will typically be at least 1 MiB.
          Additional readaheads grow linearly and increment until the per-file
          or per-system readahead cache limit on the client is reached.</para>
        <para>Readahead tunables include:</para>
        <itemizedlist>
          <listitem>
            <para><literal>llite.<replaceable>fsname_instance</replaceable>.max_read_ahead_mb</literal>
              - Controls the maximum amount of data readahead on a file.
              Files are read ahead in RPC-sized chunks (4 MiB, or the size of
              the <literal>read()</literal> call, if larger) after the second
              sequential read on a file descriptor. Random reads are done at
              the size of the <literal>read()</literal> call only (no
              readahead). Reads to non-contiguous regions of the file reset
              the readahead algorithm, and readahead is not triggered until
              sequential reads take place again.
            </para>
            <para>
              This is the global limit for all files and cannot be larger than
              1/2 of the client RAM.  To disable readahead, set
              <literal>max_read_ahead_mb=0</literal>.
            </para>
          </listitem>
          <listitem>
            <para><literal>llite.<replaceable>fsname_instance</replaceable>.max_read_ahead_per_file_mb</literal>
              - Controls the maximum number of megabytes (MiB) of data that
              should be prefetched by the client when sequential reads are
              detected on a file.  This is the per-file readahead limit and
              cannot be larger than <literal>max_read_ahead_mb</literal>.
            </para>
          </listitem>
          <listitem>
            <para><literal>llite.<replaceable>fsname_instance</replaceable>.max_read_ahead_whole_mb</literal>
              - Controls the maximum size of a file in MiB that is read in its
              entirety upon access, regardless of the size of the
              <literal>read()</literal> call.  This avoids multiple small read
              RPCs on relatively small files, when it is not possible to
              efficiently detect a sequential read pattern before the whole
              file has been read.
            </para>
            <para>The default value is the greater of 2 MiB or the size of one
              RPC, as given by <literal>max_pages_per_rpc</literal>.
            </para>
          </listitem>
        </itemizedlist>
      </section>
      <section>
        <title>Tuning Directory Statahead and AGL</title>
        <para>Many system commands, such as <literal>ls –l</literal>,
        <literal>du</literal>, and <literal>find</literal>, traverse a
        directory sequentially. To make these commands run efficiently, the
        directory statahead can be enabled to improve the performance of
        directory traversal.</para>
        <para>The statahead tunables are:</para>
        <itemizedlist>
          <listitem>
            <para><literal>statahead_max</literal> -
            Controls the maximum number of file attributes that will be
            prefetched by the statahead thread. By default, statahead is
            enabled and <literal>statahead_max</literal> is 32 files.</para>
            <para>To disable statahead, set <literal>statahead_max</literal>
            to zero via the following command on the client:</para>
            <screen>lctl set_param llite.*.statahead_max=0</screen>
            <para>To change the maximum statahead window size on a client:</para>
            <screen>lctl set_param llite.*.statahead_max=<replaceable>n</replaceable></screen>
            <para>The maximum <literal>statahead_max</literal> is 8192 files.
            </para>
            <para>The directory statahead thread will also prefetch the file
            size/block attributes from the OSTs, so that all file attributes
            are available on the client when requested by an application.
            This is controlled by the asynchronous glimpse lock (AGL) setting.
            The AGL behaviour can be disabled by setting:</para>
            <screen>lctl set_param llite.*.statahead_agl=0</screen>
          </listitem>
          <listitem>
            <para><literal>statahead_stats</literal> -
            A read-only interface that provides current statahead and AGL
            statistics, such as how many times statahead/AGL has been triggered
            since the last mount, how many statahead/AGL failures have occurred
            due to an incorrect prediction or other causes.</para>
            <note>
              <para>AGL behaviour is affected by statahead since the inodes
              processed by AGL are built by the statahead thread.  If
              statahead is disabled, then AGL is also disabled.</para>
            </note>
          </listitem>
        </itemizedlist>
      </section>
    </section>
    <section remap="h3">
      <title><indexterm>
          <primary>proc</primary>
          <secondary>read cache</secondary>
        </indexterm>Tuning OSS Read Cache</title>
      <para>The OSS read cache feature provides read-only caching of data on an OSS. This
        functionality uses the Linux page cache to store the data and uses as much physical memory
        as is allocated.</para>
      <para>OSS read cache improves Lustre file system performance in these situations:</para>
      <itemizedlist>
        <listitem>
          <para>Many clients are accessing the same data set (as in HPC applications or when
            diskless clients boot from the Lustre file system).</para>
        </listitem>
        <listitem>
          <para>One client is storing data while another client is reading it (i.e., clients are
            exchanging data via the OST).</para>
        </listitem>
        <listitem>
          <para>A client has very limited caching of its own.</para>
        </listitem>
      </itemizedlist>
      <para>OSS read cache offers these benefits:</para>
      <itemizedlist>
        <listitem>
          <para>Allows OSTs to cache read data more frequently.</para>
        </listitem>
        <listitem>
          <para>Improves repeated reads to match network speeds instead of disk speeds.</para>
        </listitem>
        <listitem>
          <para>Provides the building blocks for OST write cache (small-write aggregation).</para>
        </listitem>
      </itemizedlist>
      <section remap="h4">
        <title>Using OSS Read Cache</title>
        <para>OSS read cache is implemented on the OSS, and does not require any special support on
          the client side. Since OSS read cache uses the memory available in the Linux page cache,
          the appropriate amount of memory for the cache should be determined based on I/O patterns;
          if the data is mostly reads, then more cache is required than would be needed for mostly
          writes.</para>
        <para>OSS read cache is managed using the following tunables:</para>
        <itemizedlist>
          <listitem>
            <para><literal>read_cache_enable</literal> - Controls whether data read from disk during
              a read request is kept in memory and available for later read requests for the same
              data, without having to re-read it from disk. By default, read cache is enabled
                (<literal>read_cache_enable=1</literal>).</para>
            <para>When the OSS receives a read request from a client, it reads data from disk into
              its memory and sends the data as a reply to the request. If read cache is enabled,
              this data stays in memory after the request from the client has been fulfilled. When
              subsequent read requests for the same data are received, the OSS skips reading data
              from disk and the request is fulfilled from the cached data. The read cache is managed
              by the Linux kernel globally across all OSTs on that OSS so that the least recently
              used cache pages are dropped from memory when the amount of free memory is running
              low.</para>
            <para>If read cache is disabled (<literal>read_cache_enable=0</literal>), the OSS
              discards the data after a read request from the client is serviced and, for subsequent
              read requests, the OSS again reads the data from disk.</para>
            <para>To disable read cache on all the OSTs of an OSS, run:</para>
            <screen>root@oss1# lctl set_param obdfilter.*.read_cache_enable=0</screen>
            <para>To re-enable read cache on one OST, run:</para>
            <screen>root@oss1# lctl set_param obdfilter.{OST_name}.read_cache_enable=1</screen>
            <para>To check if read cache is enabled on all OSTs on an OSS, run:</para>
            <screen>root@oss1# lctl get_param obdfilter.*.read_cache_enable</screen>
          </listitem>
          <listitem>
            <para><literal>writethrough_cache_enable</literal> - Controls whether data sent to the
              OSS as a write request is kept in the read cache and available for later reads, or if
              it is discarded from cache when the write is completed. By default, the writethrough
              cache is enabled (<literal>writethrough_cache_enable=1</literal>).</para>
            <para>When the OSS receives write requests from a client, it receives data from the
              client into its memory and writes the data to disk. If the writethrough cache is
              enabled, this data stays in memory after the write request is completed, allowing the
              OSS to skip reading this data from disk if a later read request, or partial-page write
              request, for the same data is received.</para>
            <para>If the writethrough cache is disabled
                (<literal>writethrough_cache_enabled=0</literal>), the OSS discards the data after
              the write request from the client is completed. For subsequent read requests, or
              partial-page write requests, the OSS must re-read the data from disk.</para>
            <para>Enabling writethrough cache is advisable if clients are doing small or unaligned
              writes that would cause partial-page updates, or if the files written by one node are
              immediately being accessed by other nodes. Some examples where enabling writethrough
              cache might be useful include producer-consumer I/O models or shared-file writes with
              a different node doing I/O not aligned on 4096-byte boundaries. </para>
            <para>Disabling the writethrough cache is advisable when files are mostly written to the
              file system but are not re-read within a short time period, or files are only written
              and re-read by the same node, regardless of whether the I/O is aligned or not.</para>
            <para>To disable the writethrough cache on all OSTs of an OSS, run:</para>
            <screen>root@oss1# lctl set_param obdfilter.*.writethrough_cache_enable=0</screen>
            <para>To re-enable the writethrough cache on one OST, run:</para>
            <screen>root@oss1# lctl set_param obdfilter.{OST_name}.writethrough_cache_enable=1</screen>
            <para>To check if the writethrough cache is enabled, run:</para>
            <screen>root@oss1# lctl get_param obdfilter.*.writethrough_cache_enable</screen>
          </listitem>
          <listitem>
            <para><literal>readcache_max_filesize</literal> - Controls the maximum size of a file
              that both the read cache and writethrough cache will try to keep in memory. Files
              larger than <literal>readcache_max_filesize</literal> will not be kept in cache for
              either reads or writes.</para>
            <para>Setting this tunable can be useful for workloads where relatively small files are
              repeatedly accessed by many clients, such as job startup files, executables, log
              files, etc., but large files are read or written only once. By not putting the larger
              files into the cache, it is much more likely that more of the smaller files will
              remain in cache for a longer time.</para>
            <para>When setting <literal>readcache_max_filesize</literal>, the input value can be
              specified in bytes, or can have a suffix to indicate other binary units such as
                <literal>K</literal> (kilobytes), <literal>M</literal> (megabytes),
                <literal>G</literal> (gigabytes), <literal>T</literal> (terabytes), or
                <literal>P</literal> (petabytes).</para>
            <para>To limit the maximum cached file size to 32 MB on all OSTs of an OSS, run:</para>
            <screen>root@oss1# lctl set_param obdfilter.*.readcache_max_filesize=32M</screen>
            <para>To disable the maximum cached file size on an OST, run:</para>
            <screen>root@oss1# lctl set_param obdfilter.{OST_name}.readcache_max_filesize=-1</screen>
            <para>To check the current maximum cached file size on all OSTs of an OSS, run:</para>
            <screen>root@oss1# lctl get_param obdfilter.*.readcache_max_filesize</screen>
          </listitem>
        </itemizedlist>
      </section>
    </section>
    <section>
      <title><indexterm>
          <primary>proc</primary>
          <secondary>OSS journal</secondary>
        </indexterm>Enabling OSS Asynchronous Journal Commit</title>
      <para>The OSS asynchronous journal commit feature asynchronously writes data to disk without
        forcing a journal flush. This reduces the number of seeks and significantly improves
        performance on some hardware.</para>
      <note>
        <para>Asynchronous journal commit cannot work with direct I/O-originated writes
            (<literal>O_DIRECT</literal> flag set). In this case, a journal flush is forced. </para>
      </note>
      <para>When the asynchronous journal commit feature is enabled, client nodes keep data in the
        page cache (a page reference). Lustre clients monitor the last committed transaction number
          (<literal>transno</literal>) in messages sent from the OSS to the clients. When a client
        sees that the last committed <literal>transno</literal> reported by the OSS is at least
        equal to the bulk write <literal>transno</literal>, it releases the reference on the
        corresponding pages. To avoid page references being held for too long on clients after a
        bulk write, a 7 second ping request is scheduled (the default OSS file system commit time
        interval is 5 seconds) after the bulk write reply is received, so the OSS has an opportunity
        to report the last committed <literal>transno</literal>.</para>
      <para>If the OSS crashes before the journal commit occurs, then intermediate data is lost.
        However, OSS recovery functionality incorporated into the asynchronous journal commit
        feature causes clients to replay their write requests and compensate for the missing disk
        updates by restoring the state of the file system.</para>
      <para>By default, <literal>sync_journal</literal> is enabled
          (<literal>sync_journal=1</literal>), so that journal entries are committed synchronously.
        To enable asynchronous journal commit, set the <literal>sync_journal</literal> parameter to
          <literal>0</literal> by entering: </para>
      <screen>$ lctl set_param obdfilter.*.sync_journal=0 
obdfilter.lol-OST0001.sync_journal=0</screen>
      <para>An associated <literal>sync-on-lock-cancel</literal> feature (enabled by default)
        addresses a data consistency issue that can result if an OSS crashes after multiple clients
        have written data into intersecting regions of an object, and then one of the clients also
        crashes. A condition is created in which the POSIX requirement for continuous writes is
        violated along with a potential for corrupted data. With
          <literal>sync-on-lock-cancel</literal> enabled, if a cancelled lock has any volatile
        writes attached to it, the OSS synchronously writes the journal to disk on lock
        cancellation. Disabling the <literal>sync-on-lock-cancel</literal> feature may enhance
        performance for concurrent write workloads, but it is recommended that you not disable this
        feature.</para>
      <para> The <literal>sync_on_lock_cancel</literal> parameter can be set to the following
        values:</para>
      <itemizedlist>
        <listitem>
          <para><literal>always</literal> - Always force a journal flush on lock cancellation
            (default when <literal>async_journal</literal> is enabled).</para>
        </listitem>
        <listitem>
          <para><literal>blocking</literal> - Force a journal flush only when the local cancellation
            is due to a blocking callback.</para>
        </listitem>
        <listitem>
          <para><literal>never</literal> - Do not force any journal flush (default when
              <literal>async_journal</literal> is disabled).</para>
        </listitem>
      </itemizedlist>
      <para>For example, to set <literal>sync_on_lock_cancel</literal> to not to force a journal
        flush, use a command similar to:</para>
      <screen>$ lctl get_param obdfilter.*.sync_on_lock_cancel
obdfilter.lol-OST0001.sync_on_lock_cancel=never</screen>
    </section>
    <section xml:id="dbdoclet.TuningModRPCs" condition='l28'>
      <title>
        <indexterm>
          <primary>proc</primary>
          <secondary>client metadata performance</secondary>
        </indexterm>
        Tuning the Client Metadata RPC Stream
      </title>
      <para>The client metadata RPC stream represents the metadata RPCs issued
        in parallel by a client to a MDT target. The metadata RPCs can be split
        in two categories: the requests that do not modify the file system
        (like getattr operation), and the requests that do modify the file system
        (like create, unlink, setattr operations). To help optimize the client
        metadata RPC stream, several tuning variables are provided to adjust
        behavior according to network conditions and cluster size.</para>
      <para>Note that increasing the number of metadata RPCs issued in parallel
        might improve the performance metadata intensive parallel applications,
        but as a consequence it will consume more memory on the client and on
        the MDS.</para>
      <section>
        <title>Configuring the Client Metadata RPC Stream</title>
        <para>The MDC <literal>max_rpcs_in_flight</literal> parameter defines
          the maximum number of metadata RPCs, both modifying and
          non-modifying RPCs, that can be sent in parallel by a client to a MDT
          target. This includes every file system metadata operations, such as
          file or directory stat, creation, unlink. The default setting is 8,
          minimum setting is 1 and maximum setting is 256.</para>
        <para>To set the <literal>max_rpcs_in_flight</literal> parameter, run
          the following command on the Lustre client:</para>
        <screen>client$ lctl set_param mdc.*.max_rpcs_in_flight=16</screen>
        <para>The MDC <literal>max_mod_rpcs_in_flight</literal> parameter
          defines the maximum number of file system modifying RPCs that can be
          sent in parallel by a client to a MDT target. For example, the Lustre
          client sends modify RPCs when it performs file or directory creation,
          unlink, access permission modification or ownership modification. The
          default setting is 7, minimum setting is 1 and maximum setting is
          256.</para>
        <para>To set the <literal>max_mod_rpcs_in_flight</literal> parameter,
          run the following command on the Lustre client:</para>
        <screen>client$ lctl set_param mdc.*.max_mod_rpcs_in_flight=12</screen>
        <para>The <literal>max_mod_rpcs_in_flight</literal> value must be
          strictly less than the <literal>max_rpcs_in_flight</literal> value.
          It must also be less or equal to the MDT
          <literal>max_mod_rpcs_per_client</literal> value. If one of theses
          conditions is not enforced, the setting fails and an explicit message
          is written in the Lustre log.</para>
        <para>The MDT <literal>max_mod_rpcs_per_client</literal> parameter is a
          tunable of the kernel module <literal>mdt</literal> that defines the
          maximum number of file system modifying RPCs in flight allowed per
          client. The parameter can be updated at runtime, but the change is
          effective to new client connections only. The default setting is 8.
        </para>
        <para>To set the <literal>max_mod_rpcs_per_client</literal> parameter,
          run the following command on the MDS:</para>
        <screen>mds$ echo 12 > /sys/module/mdt/parameters/max_mod_rpcs_per_client</screen>
      </section>
      <section>
        <title>Monitoring the Client Metadata RPC Stream</title>
        <para>The <literal>rpc_stats</literal> file contains histogram data
          showing information about modify metadata RPCs. It can be helpful to
          identify the level of parallelism achieved by an application doing
          modify metadata operations.</para>
        <para><emphasis role="bold">Example:</emphasis></para>
        <screen>client$ lctl get_param mdc.*.rpc_stats
snapshot_time:         1441876896.567070 (secs.usecs)
modify_RPCs_in_flight:  0

                        modify
rpcs in flight        rpcs   % cum %
0:                       0   0   0
1:                      56   0   0
2:                      40   0   0
3:                      70   0   0
4                       41   0   0
5:                      51   0   1
6:                      88   0   1
7:                     366   1   2
8:                    1321   5   8
9:                    3624  15  23
10:                   6482  27  50
11:                   7321  30  81
12:                   4540  18 100</screen>
        <para>The file information includes:</para>
        <itemizedlist>
          <listitem>
            <para><literal>snapshot_time</literal> - UNIX epoch instant the
              file was read.</para>
          </listitem>
          <listitem>
            <para><literal>modify_RPCs_in_flight</literal> - Number of modify
              RPCs issued by the MDC, but not completed at the time of the
              snapshot. This value should always be less than or equal to
              <literal>max_mod_rpcs_in_flight</literal>.</para>
          </listitem>
          <listitem>
            <para><literal>rpcs in flight</literal> - Number of modify RPCs
              that are pending when a RPC is sent, the relative percentage
              (<literal>%</literal>) of total modify RPCs, and the cumulative
              percentage (<literal>cum %</literal>) to that point.</para>
          </listitem>
        </itemizedlist>
        <para>If a large proportion of modify metadata RPCs are issued with a
          number of pending metadata RPCs close to the
          <literal>max_mod_rpcs_in_flight</literal> value, it means the
          <literal>max_mod_rpcs_in_flight</literal> value could be increased to
          improve the modify metadata performance.</para>
      </section>
    </section>
  </section>
  <section>
    <title>Configuring Timeouts in a Lustre File System</title>
    <para>In a Lustre file system, RPC timeouts are set using an adaptive timeouts mechanism, which
      is enabled by default. Servers track RPC completion times and then report back to clients
      estimates for completion times for future RPCs. Clients  use these estimates to set RPC
      timeout values. If the processing of server requests slows down for any reason, the server
      estimates for RPC completion increase, and clients then revise RPC timeout values to allow
      more time for RPC completion.</para>
    <para>If the RPCs queued on the server approach the RPC timeout specified by the client, to
      avoid RPC timeouts and disconnect/reconnect cycles, the server sends an "early reply" to the
      client, telling the client to allow more time. Conversely, as server processing speeds up, RPC
      timeout values decrease, resulting in faster detection if the server becomes non-responsive
      and quicker connection to the failover partner of the server.</para>
    <section>
      <title><indexterm>
          <primary>proc</primary>
          <secondary>configuring adaptive timeouts</secondary>
        </indexterm><indexterm>
          <primary>configuring</primary>
          <secondary>adaptive timeouts</secondary>
        </indexterm><indexterm>
          <primary>proc</primary>
          <secondary>adaptive timeouts</secondary>
        </indexterm>Configuring Adaptive Timeouts</title>
      <para>The adaptive timeout parameters in the table below can be set persistently system-wide
        using <literal>lctl conf_param</literal> on the MGS. For example, the following command sets
        the <literal>at_max</literal> value  for all servers and clients associated with the file
        system
        <literal>testfs</literal>:<screen>lctl conf_param testfs.sys.at_max=1500</screen></para>
      <note>
        <para>Clients that access multiple Lustre file systems must use the same parameter values
          for all file systems.</para>
      </note>
      <informaltable frame="all">
        <tgroup cols="2">
          <colspec colname="c1" colwidth="30*"/>
          <colspec colname="c2" colwidth="80*"/>
          <thead>
            <row>
              <entry>
                <para><emphasis role="bold">Parameter</emphasis></para>
              </entry>
              <entry>
                <para><emphasis role="bold">Description</emphasis></para>
              </entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>
                <para>
                  <literal> at_min </literal></para>
              </entry>
              <entry>
                <para>Minimum adaptive timeout (in seconds). The default value is 0. The
                    <literal>at_min</literal> parameter is the minimum processing time that a server
                  will report. Ideally, <literal>at_min</literal> should be set to its default
                  value. Clients base their timeouts on this value, but they do not use this value
                  directly. </para>
                <para>If, for unknown reasons (usually due to temporary network outages), the
                  adaptive timeout value is too short and clients time out their RPCs, you can
                  increase the <literal>at_min</literal> value to compensate for this.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal> at_max </literal></para>
              </entry>
              <entry>
                <para>Maximum adaptive timeout (in seconds). The <literal>at_max</literal> parameter
                  is an upper-limit on the service time estimate. If <literal>at_max</literal> is
                  reached, an RPC request times out.</para>
                <para>Setting <literal>at_max</literal> to 0 causes adaptive timeouts to be disabled
                  and a fixed timeout method to be used instead (see <xref
                    xmlns:xlink="http://www.w3.org/1999/xlink" linkend="section_c24_nt5_dl"/></para>
                <note>
                  <para>If slow hardware causes the service estimate to increase beyond the default
                    value of <literal>at_max</literal>, increase <literal>at_max</literal> to the
                    maximum time you are willing to wait for an RPC completion.</para>
                </note>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal> at_history </literal></para>
              </entry>
              <entry>
                <para>Time period (in seconds) within which adaptive timeouts remember the slowest
                  event that occurred. The default is 600.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal> at_early_margin </literal></para>
              </entry>
              <entry>
                <para>Amount of time before the Lustre server sends an early reply (in seconds).
                  Default is 5.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal> at_extra </literal></para>
              </entry>
              <entry>
                <para>Incremental amount of time that a server requests with each early reply (in
                  seconds). The server does not know how much time the RPC will take, so it asks for
                  a fixed value. The default is 30, which provides a balance between sending too
                  many early replies for the same RPC and overestimating the actual completion
                  time.</para>
                <para>When a server finds a queued request about to time out and needs to send an
                  early reply out, the server adds the <literal>at_extra</literal> value. If the
                  time expires, the Lustre server drops the request, and the client enters recovery
                  status and reconnects to restore the connection to normal status.</para>
                <para>If you see multiple early replies for the same RPC asking for 30-second
                  increases, change the <literal>at_extra</literal> value to a larger number to cut
                  down on early replies sent and, therefore, network load.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal> ldlm_enqueue_min </literal></para>
              </entry>
              <entry>
                <para>Minimum lock enqueue time (in seconds). The default is 100. The time it takes
                  to enqueue a lock, <literal>ldlm_enqueue</literal>, is the maximum of the measured
                  enqueue estimate (influenced by <literal>at_min</literal> and
                    <literal>at_max</literal> parameters), multiplied by a weighting factor and the
                  value of <literal>ldlm_enqueue_min</literal>. </para>
                <para>Lustre Distributed Lock Manager (LDLM) lock enqueues have a dedicated minimum
                  value for <literal>ldlm_enqueue_min</literal>. Lock enqueue timeouts increase as
                  the measured enqueue times increase (similar to adaptive timeouts).</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>
      <section>
        <title>Interpreting Adaptive Timeout Information</title>
        <para>Adaptive timeout information can be obtained from the <literal>timeouts</literal>
          files in <literal>/proc/fs/lustre/*/</literal> on each server and client using the
            <literal>lctl</literal> command. To read information from a <literal>timeouts</literal>
          file, enter a command similar to:</para>
        <screen># lctl get_param -n ost.*.ost_io.timeouts
service : cur 33  worst 34 (at 1193427052, 0d0h26m40s ago) 1 1 33 2</screen>
        <para>In this example, the <literal>ost_io</literal> service on this node is currently
          reporting an estimated RPC service time of 33 seconds. The worst RPC service time was 34
          seconds, which occurred 26 minutes ago.</para>
        <para>The output also provides a history of service times. Four &quot;bins&quot; of adaptive
          timeout history are shown, with the maximum RPC time in each bin reported. In both the
          0-150s bin and the 150-300s bin, the maximum RPC time was 1. The 300-450s bin shows the
          worst (maximum) RPC time at 33 seconds, and the 450-600s bin shows a maximum of RPC time
          of 2 seconds. The estimated service time is the maximum value across the four bins (33
          seconds in this example).</para>
        <para>Service times (as reported by the servers) are also tracked in the client OBDs, as
          shown in this example:</para>
        <screen># lctl get_param osc.*.timeouts
last reply : 1193428639, 0d0h00m00s ago
network    : cur  1 worst  2 (at 1193427053, 0d0h26m26s ago)  1  1  1  1
portal 6   : cur 33 worst 34 (at 1193427052, 0d0h26m27s ago) 33 33 33  2
portal 28  : cur  1 worst  1 (at 1193426141, 0d0h41m38s ago)  1  1  1  1
portal 7   : cur  1 worst  1 (at 1193426141, 0d0h41m38s ago)  1  0  1  1
portal 17  : cur  1 worst  1 (at 1193426177, 0d0h41m02s ago)  1  0  0  1
</screen>
        <para>In this example, portal 6, the <literal>ost_io</literal> service portal, shows the
          history of service estimates reported by the portal.</para>
        <para>Server statistic files also show the range of estimates including min, max, sum, and
          sumsq. For example:</para>
        <screen># lctl get_param mdt.*.mdt.stats
...
req_timeout               6 samples [sec] 1 10 15 105
...
</screen>
      </section>
    </section>
    <section xml:id="section_c24_nt5_dl">
      <title>Setting Static Timeouts<indexterm>
          <primary>proc</primary>
          <secondary>static timeouts</secondary>
        </indexterm></title>
      <para>The Lustre software provides two sets of static (fixed) timeouts, LND timeouts and
        Lustre timeouts, which are used when adaptive timeouts are not enabled.</para>
      <para>
        <itemizedlist>
          <listitem>
            <para><emphasis role="italic"><emphasis role="bold">LND timeouts</emphasis></emphasis> -
              LND timeouts ensure that point-to-point communications across a network complete in a
              finite time in the presence of failures, such as packages lost or broken connections.
              LND timeout parameters are set for each individual LND.</para>
            <para>LND timeouts are logged with the <literal>S_LND</literal> flag set. They are not
              printed as console messages, so check the Lustre log for <literal>D_NETERROR</literal>
              messages or enable printing of <literal>D_NETERROR</literal> messages to the console
              using:<screen>lctl set_param printk=+neterror</screen></para>
            <para>Congested routers can be a source of spurious LND timeouts. To avoid this
              situation, increase the number of LNet router buffers to reduce back-pressure and/or
              increase LND timeouts on all nodes on all connected networks. Also consider increasing
              the total number of LNet router nodes in the system so that the aggregate router
              bandwidth matches the aggregate server bandwidth.</para>
          </listitem>
          <listitem>
            <para><emphasis role="italic"><emphasis role="bold">Lustre timeouts
                </emphasis></emphasis>- Lustre timeouts ensure that Lustre RPCs complete in a finite
              time in the presence of failures when adaptive timeouts are not enabled. Adaptive
              timeouts are enabled by default. To disable adaptive timeouts at run time, set
                <literal>at_max</literal> to 0 by running on the
              MGS:<screen># lctl conf_param <replaceable>fsname</replaceable>.sys.at_max=0</screen></para>
            <note>
              <para>Changing the status of adaptive timeouts at runtime may cause a transient client
                timeout, recovery, and reconnection.</para>
            </note>
            <para>Lustre timeouts are always printed as console messages. </para>
            <para>If Lustre timeouts are not accompanied by LND timeouts, increase the Lustre
              timeout on both servers and clients. Lustre timeouts are set using a command such as
              the following:<screen># lctl set_param timeout=30</screen></para>
            <para>Lustre timeout parameters are described in the table below.</para>
          </listitem>
        </itemizedlist>
        <informaltable frame="all">
          <tgroup cols="2">
            <colspec colname="c1" colnum="1" colwidth="30*"/>
            <colspec colname="c2" colnum="2" colwidth="70*"/>
            <thead>
              <row>
                <entry>Parameter</entry>
                <entry>Description</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry><literal>timeout</literal></entry>
                <entry>
                  <para>The time that a client waits for a server to complete an RPC (default 100s).
                    Servers wait half this time for a normal client RPC to complete and a quarter of
                    this time for a single bulk request (read or write of up to 4 MB) to complete.
                    The client pings recoverable targets (MDS and OSTs) at one quarter of the
                    timeout, and the server waits one and a half times the timeout before evicting a
                    client for being &quot;stale.&quot;</para>
                  <para>Lustre client sends periodic &apos;ping&apos; messages to servers with which
                    it has had no communication for the specified period of time. Any network
                    activity between a client and a server in the file system also serves as a
                    ping.</para>
                </entry>
              </row>
              <row>
                <entry><literal>ldlm_timeout</literal></entry>
                <entry>
                  <para>The time that a server waits for a client to reply to an initial AST (lock
                    cancellation request). The default is 20s for an OST and 6s for an MDS. If the
                    client replies to the AST, the server will give it a normal timeout (half the
                    client timeout) to flush any dirty data and release the lock.</para>
                </entry>
              </row>
              <row>
                <entry><literal>fail_loc</literal></entry>
                <entry>
                  <para>An internal debugging failure hook. The default value of
                      <literal>0</literal> means that no failure will be triggered or
                    injected.</para>
                </entry>
              </row>
              <row>
                <entry><literal>dump_on_timeout</literal></entry>
                <entry>
                  <para>Triggers a dump of the Lustre debug log when a timeout occurs. The default
                    value of <literal>0</literal> (zero) means a dump of the Lustre debug log will
                    not be triggered.</para>
                </entry>
              </row>
              <row>
                <entry><literal>dump_on_eviction</literal></entry>
                <entry>
                  <para>Triggers a dump of the Lustre debug log when an eviction occurs. The default
                    value of <literal>0</literal> (zero) means a dump of the Lustre debug log will
                    not be triggered. </para>
                </entry>
              </row>
            </tbody>
          </tgroup>
        </informaltable>
      </para>
    </section>
  </section>
  <section remap="h3">
    <title><indexterm>
        <primary>proc</primary>
        <secondary>LNet</secondary>
      </indexterm><indexterm>
        <primary>LNet</primary>
        <secondary>proc</secondary>
      </indexterm>Monitoring LNet</title>
    <para>LNet information is located in <literal>/proc/sys/lnet</literal> in these files:<itemizedlist>
        <listitem>
          <para><literal>peers</literal> - Shows all NIDs known to this node and provides
            information on the queue state.</para>
          <para>Example:</para>
          <screen># lctl get_param peers
nid                refs   state  max  rtr  min   tx    min   queue
0@lo               1      ~rtr   0    0    0     0     0     0
192.168.10.35@tcp  1      ~rtr   8    8    8     8     6     0
192.168.10.36@tcp  1      ~rtr   8    8    8     8     6     0
192.168.10.37@tcp  1      ~rtr   8    8    8     8     6     0</screen>
          <para>The fields are explained in the table below:</para>
          <informaltable frame="all">
            <tgroup cols="2">
              <colspec colname="c1" colwidth="30*"/>
              <colspec colname="c2" colwidth="80*"/>
              <thead>
                <row>
                  <entry>
                    <para><emphasis role="bold">Field</emphasis></para>
                  </entry>
                  <entry>
                    <para><emphasis role="bold">Description</emphasis></para>
                  </entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <para>
                      <literal>refs</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>A reference count. </para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>state</literal>
                    </para>
                  </entry>
                  <entry>
                    <para>If the node is a router, indicates the state of the router. Possible
                      values are:</para>
                    <itemizedlist>
                      <listitem>
                        <para><literal>NA</literal> - Indicates the node is not a router.</para>
                      </listitem>
                      <listitem>
                        <para><literal>up/down</literal>- Indicates if the node (router) is up or
                          down.</para>
                      </listitem>
                    </itemizedlist>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>max </literal></para>
                  </entry>
                  <entry>
                    <para>Maximum number of concurrent sends from this peer.</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>rtr </literal></para>
                  </entry>
                  <entry>
                    <para>Number of routing buffer credits.</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>min </literal></para>
                  </entry>
                  <entry>
                    <para>Minimum number of routing buffer credits seen.</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>tx </literal></para>
                  </entry>
                  <entry>
                    <para>Number of send credits.</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>min </literal></para>
                  </entry>
                  <entry>
                    <para>Minimum number of send credits seen.</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal>queue </literal></para>
                  </entry>
                  <entry>
                    <para>Total bytes in active/queued sends.</para>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
          <para>Credits are initialized to allow a certain number of operations (in the example
            above the table, eight as shown in the <literal>max</literal> column. LNet keeps track
            of the minimum number of credits ever seen over time showing the peak congestion that
            has occurred during the time monitored. Fewer available credits indicates a more
            congested resource. </para>
          <para>The number of credits currently in flight (number of transmit credits) is shown in
            the <literal>tx</literal> column. The maximum number of send credits available is shown
            in the <literal>max</literal> column and never changes. The number of router buffers
            available for consumption by a peer is shown in the <literal>rtr</literal>
            column.</para>
          <para>Therefore, <literal>rtr</literal> – <literal>tx</literal> is the number of transmits
            in flight. Typically, <literal>rtr == max</literal>, although a configuration can be set
            such that <literal>max >= rtr</literal>. The ratio of routing buffer credits to send
            credits (<literal>rtr/tx</literal>) that is less than <literal>max</literal> indicates
            operations are in progress. If the ratio <literal>rtr/tx</literal> is greater than
              <literal>max</literal>, operations are blocking.</para>
          <para>LNet also limits concurrent sends and number of router buffers allocated to a single
            peer so that no peer can occupy all these resources.</para>
        </listitem>
        <listitem>
          <para><literal>nis</literal> - Shows the current queue health on this node.</para>
          <para>Example:</para>
          <screen># lctl get_param nis
nid                    refs   peer    max   tx    min
0@lo                   3      0       0     0     0
192.168.10.34@tcp      4      8       256   256   252
</screen>
          <para> The fields are explained in the table below.</para>
          <informaltable frame="all">
            <tgroup cols="2">
              <colspec colname="c1" colwidth="30*"/>
              <colspec colname="c2" colwidth="80*"/>
              <thead>
                <row>
                  <entry>
                    <para><emphasis role="bold">Field</emphasis></para>
                  </entry>
                  <entry>
                    <para><emphasis role="bold">Description</emphasis></para>
                  </entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <para>
                      <literal> nid </literal></para>
                  </entry>
                  <entry>
                    <para>Network interface.</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal> refs </literal></para>
                  </entry>
                  <entry>
                    <para>Internal reference counter.</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal> peer </literal></para>
                  </entry>
                  <entry>
                    <para>Number of peer-to-peer send credits on this NID. Credits are used to size
                      buffer pools.</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal> max </literal></para>
                  </entry>
                  <entry>
                    <para>Total number of send credits on this NID.</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal> tx </literal></para>
                  </entry>
                  <entry>
                    <para>Current number of send credits available on this NID.</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal> min </literal></para>
                  </entry>
                  <entry>
                    <para>Lowest number of send credits available on this NID.</para>
                  </entry>
                </row>
                <row>
                  <entry>
                    <para>
                      <literal> queue </literal></para>
                  </entry>
                  <entry>
                    <para>Total bytes in active/queued sends.</para>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </informaltable>
          <para><emphasis role="bold"><emphasis role="italic">Analysis:</emphasis></emphasis></para>
          <para>Subtracting <literal>max</literal> from <literal>tx</literal>
              (<literal>max</literal> - <literal>tx</literal>) yields the number of sends currently
            active. A large or increasing number of active sends may indicate a problem.</para>
        </listitem>
      </itemizedlist></para>
  </section>
  <section remap="h3" xml:id="dbdoclet.balancing_free_space">
    <title><indexterm>
        <primary>proc</primary>
        <secondary>free space</secondary>
      </indexterm>Allocating Free Space on OSTs</title>
    <para>Free space is allocated using either a round-robin or a weighted
    algorithm. The allocation method is determined by the maximum amount of
    free-space imbalance between the OSTs. When free space is relatively
    balanced across OSTs, the faster round-robin allocator is used, which
    maximizes network balancing. The weighted allocator is used when any two
    OSTs are out of balance by more than a specified threshold.</para>
    <para>Free space distribution can be tuned using these two
    <literal>/proc</literal> tunables:</para>
    <itemizedlist>
      <listitem>
        <para><literal>qos_threshold_rr</literal> - The threshold at which
        the allocation method switches from round-robin to weighted is set
        in this file. The default is to switch to the weighted algorithm when
        any two OSTs are out of balance by more than 17 percent.</para>
      </listitem>
      <listitem>
        <para><literal>qos_prio_free</literal> - The weighting priority used
        by the weighted allocator can be adjusted in this file. Increasing the
        value of <literal>qos_prio_free</literal> puts more weighting on the
        amount of free space available on each OST and less on how stripes are
        distributed across OSTs. The default value is 91 percent weighting for
        free space rebalancing and 9 percent for OST balancing. When the
        free space priority is set to 100, weighting is based entirely on free
        space and location is no longer used by the striping algorithm.</para>
      </listitem>
      <listitem>
        <para condition="l29"><literal>reserved_mb_low</literal> - The low
        watermark used to stop object allocation if available space is less
        than it. The default is 0.1 percent of total OST size.</para>
      </listitem>
       <listitem>
        <para condition="l29"><literal>reserved_mb_high</literal> - The high watermark used to start
          object allocation if available space is more than it. The default is 0.2 percent of total
          OST size.</para>
      </listitem>
    </itemizedlist>
    <para>For more information about monitoring and managing free space, see <xref
        xmlns:xlink="http://www.w3.org/1999/xlink" linkend="dbdoclet.50438209_10424"/>.</para>
  </section>
  <section remap="h3">
    <title><indexterm>
        <primary>proc</primary>
        <secondary>locking</secondary>
      </indexterm>Configuring Locking</title>
    <para>The <literal>lru_size</literal> parameter is used to control the
      number of client-side locks in the LRU cached locks queue. LRU size is
      normally dynamic, based on load to optimize the number of locks cached
      on nodes that have different workloads (e.g., login/build nodes vs.
      compute nodes vs. backup nodes).</para>
    <para>The total number of locks available is a function of the server RAM.
      The default limit is 50 locks/1 MB of RAM. If memory pressure is too high,
      the LRU size is shrunk. The number of locks on the server is limited to
      <replaceable>num_osts_per_oss * num_clients * lru_size</replaceable>
      as follows: </para>
    <itemizedlist>
      <listitem>
        <para>To enable automatic LRU sizing, set the
	<literal>lru_size</literal> parameter to 0. In this case, the
	<literal>lru_size</literal> parameter shows the current number of locks
        being used on the client. Dynamic LRU resizing is enabled by default.
	</para>
      </listitem>
      <listitem>
        <para>To specify a maximum number of locks, set the
	<literal>lru_size</literal> parameter to a value other than zero.
	A good default value for compute nodes is around
	<literal>100 * <replaceable>num_cpus</replaceable></literal>.
        It is recommended that you only set <literal>lru_size</literal>
	to be signifivantly larger on a few login nodes where multiple
	users access the file system interactively.</para>
      </listitem>
    </itemizedlist>
    <para>To clear the LRU on a single client, and, as a result, flush client
      cache without changing the <literal>lru_size</literal> value, run:</para>
    <screen># lctl set_param ldlm.namespaces.<replaceable>osc_name|mdc_name</replaceable>.lru_size=clear</screen>
    <para>If the LRU size is set lower than the number of existing locks,
      <emphasis>unused</emphasis> locks are canceled immediately. Use
      <literal>clear</literal> to cancel all locks without changing the value.
    </para>
    <note>
      <para>The <literal>lru_size</literal> parameter can only be set
        temporarily using <literal>lctl set_param</literal>, it cannot be set
	permanently.</para>
    </note>
    <para>To disable dynamic LRU resizing on the clients, run for example:
    </para>
    <screen># lctl set_param ldlm.namespaces.*osc*.lru_size=5000</screen>
    <para>To determine the number of locks being granted with dynamic LRU
      resizing, run:</para>
    <screen>$ lctl get_param ldlm.namespaces.*.pool.limit</screen>
    <para>The <literal>lru_max_age</literal> parameter is used to control the
      age of client-side locks in the LRU cached locks queue. This limits how
      long unused locks are cached on the client, and avoids idle clients from
      holding locks for an excessive time, which reduces memory usage on both
      the client and server, as well as reducing work during server recovery.
    </para>
    <para>The <literal>lru_max_age</literal> is set and printed in milliseconds,
      and by default is 3900000 ms (65 minutes).</para>
    <para condition='l2B'>Since Lustre 2.11, in addition to setting the
      maximum lock age in milliseconds, it can also be set using a suffix of
      <literal>s</literal> or <literal>ms</literal> to indicate seconds or
      milliseconds, respectively.  For example to set the client's maximum
      lock age to 15 minutes (900s) run:
    </para>
    <screen>
# lctl set_param ldlm.namespaces.*MDT*.lru_max_age=900s
# lctl get_param ldlm.namespaces.*MDT*.lru_max_age
ldlm.namespaces.myth-MDT0000-mdc-ffff8804296c2800.lru_max_age=900000
    </screen>
  </section>
  <section xml:id="dbdoclet.50438271_87260">
    <title><indexterm>
        <primary>proc</primary>
        <secondary>thread counts</secondary>
      </indexterm>Setting MDS and OSS Thread Counts</title>
    <para>MDS and OSS thread counts tunable can be used to set the minimum and maximum thread counts
      or get the current number of running threads for the services listed in the table
      below.</para>
    <informaltable frame="all">
      <tgroup cols="2">
        <colspec colname="c1" colwidth="50*"/>
        <colspec colname="c2" colwidth="50*"/>
        <tbody>
          <row>
            <entry>
              <para>
                <emphasis role="bold">Service</emphasis></para>
            </entry>
            <entry>
              <para>
                <emphasis role="bold">Description</emphasis></para>
            </entry>
          </row>
          <row>
            <entry>
              <literal> mds.MDS.mdt </literal>
            </entry>
            <entry>
              <para>Main metadata operations service</para>
            </entry>
          </row>
          <row>
            <entry>
              <literal> mds.MDS.mdt_readpage </literal>
            </entry>
            <entry>
              <para>Metadata <literal>readdir</literal> service</para>
            </entry>
          </row>
          <row>
            <entry>
              <literal> mds.MDS.mdt_setattr </literal>
            </entry>
            <entry>
              <para>Metadata <literal>setattr/close</literal> operations service </para>
            </entry>
          </row>
          <row>
            <entry>
              <literal> ost.OSS.ost </literal>
            </entry>
            <entry>
              <para>Main data operations service</para>
            </entry>
          </row>
          <row>
            <entry>
              <literal> ost.OSS.ost_io </literal>
            </entry>
            <entry>
              <para>Bulk data I/O services</para>
            </entry>
          </row>
          <row>
            <entry>
              <literal> ost.OSS.ost_create </literal>
            </entry>
            <entry>
              <para>OST object pre-creation service</para>
            </entry>
          </row>
          <row>
            <entry>
              <literal> ldlm.services.ldlm_canceld </literal>
            </entry>
            <entry>
              <para>DLM lock cancel service</para>
            </entry>
          </row>
          <row>
            <entry>
              <literal> ldlm.services.ldlm_cbd </literal>
            </entry>
            <entry>
              <para>DLM lock grant service</para>
            </entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>
    <para>For each service, an entry as shown below is
      created:<screen>/proc/fs/lustre/<replaceable>service</replaceable>/*/threads_<replaceable>min|max|started</replaceable></screen></para>
    <itemizedlist>
      <listitem>
        <para>To temporarily set this tunable, run:</para>
        <screen># lctl <replaceable>get|set</replaceable>_param <replaceable>service</replaceable>.threads_<replaceable>min|max|started</replaceable> </screen>
        </listitem>
      <listitem>
        <para>To permanently set this tunable, run:</para>
	<screen># lctl conf_param <replaceable>obdname|fsname.obdtype</replaceable>.threads_<replaceable>min|max|started</replaceable> </screen>
	<para condition='l25'>For version 2.5 or later, run:
		<screen># lctl set_param -P <replaceable>service</replaceable>.threads_<replaceable>min|max|started</replaceable></screen></para>
      </listitem>
    </itemizedlist>
      <para>The following examples show how to set thread counts and get the number of running threads
        for the service <literal>ost_io</literal>  using the tunable
	<literal><replaceable>service</replaceable>.threads_<replaceable>min|max|started</replaceable></literal>.</para>
    <itemizedlist>
      <listitem>
        <para>To get the number of running threads, run:</para>
        <screen># lctl get_param ost.OSS.ost_io.threads_started
ost.OSS.ost_io.threads_started=128</screen>
      </listitem>
      <listitem>
        <para>To set the number of threads to the maximum value (512), run:</para>
        <screen># lctl get_param ost.OSS.ost_io.threads_max
ost.OSS.ost_io.threads_max=512</screen>
      </listitem>
      <listitem>
        <para>To set the maximum thread count to 256 instead of 512 (to avoid overloading the
          storage or for an array with requests), run:</para>
        <screen># lctl set_param ost.OSS.ost_io.threads_max=256
ost.OSS.ost_io.threads_max=256</screen>
      </listitem>
      <listitem>
        <para>To set the maximum thread count to 256 instead of 512 permanently, run:</para>
        <screen># lctl conf_param testfs.ost.ost_io.threads_max=256</screen>
	<para condition='l25'>For version 2.5 or later, run:
	<screen># lctl set_param -P ost.OSS.ost_io.threads_max=256
ost.OSS.ost_io.threads_max=256 </screen> </para>
      </listitem>
      <listitem>
        <para> To check if the <literal>threads_max</literal> setting is active, run:</para>
        <screen># lctl get_param ost.OSS.ost_io.threads_max
ost.OSS.ost_io.threads_max=256</screen>
      </listitem>
    </itemizedlist>
    <note>
      <para>If the number of service threads is changed while the file system is running, the change
        may not take effect until the file system is stopped and rest. If the number of service
        threads in use exceeds the new <literal>threads_max</literal> value setting, service threads
        that are already running will not be stopped.</para>
    </note>
    <para>See also <xref xmlns:xlink="http://www.w3.org/1999/xlink" linkend="lustretuning"/></para>
  </section>
  <section xml:id="dbdoclet.50438271_83523">
    <title><indexterm>
        <primary>proc</primary>
        <secondary>debug</secondary>
      </indexterm>Enabling and Interpreting Debugging Logs</title>
    <para>By default, a detailed log of all operations is generated to aid in debugging. Flags that
      control debugging are found in <literal>/proc/sys/lnet/debug</literal>. </para>
    <para>The overhead of debugging can affect the performance of Lustre file system. Therefore, to
      minimize the impact on performance, the debug level can be lowered, which affects the amount
      of debugging information kept in the internal log buffer but does not alter the amount of
      information to goes into syslog. You can raise the debug level when you need to collect logs
      to debug problems. </para>
    <para>The debugging mask can be set using &quot;symbolic names&quot;. The symbolic format is
      shown in the examples below.<itemizedlist>
        <listitem>
          <para>To verify the debug level used, examine the <literal>sysctl</literal> that controls
            debugging by running:</para>
          <screen># sysctl lnet.debug 
lnet.debug = ioctl neterror warning error emerg ha config console</screen>
        </listitem>
        <listitem>
          <para>To turn off debugging (except for network error debugging), run the following
            command on all nodes concerned:</para>
          <screen># sysctl -w lnet.debug=&quot;neterror&quot; 
lnet.debug = neterror</screen>
        </listitem>
      </itemizedlist><itemizedlist>
        <listitem>
          <para>To turn off debugging completely, run the following command on all nodes
            concerned:</para>
          <screen># sysctl -w lnet.debug=0 
lnet.debug = 0</screen>
        </listitem>
        <listitem>
          <para>To set an appropriate debug level for a production environment, run:</para>
          <screen># sysctl -w lnet.debug=&quot;warning dlmtrace error emerg ha rpctrace vfstrace&quot; 
lnet.debug = warning dlmtrace error emerg ha rpctrace vfstrace</screen>
          <para>The flags shown in this example collect enough high-level information to aid
            debugging, but they do not cause any serious performance impact.</para>
        </listitem>
      </itemizedlist><itemizedlist>
        <listitem>
          <para>To clear all flags and set new flags, run:</para>
          <screen># sysctl -w lnet.debug=&quot;warning&quot; 
lnet.debug = warning</screen>
        </listitem>
      </itemizedlist><itemizedlist>
        <listitem>
          <para>To add new flags to flags that have already been set, precede each one with a
              &quot;<literal>+</literal>&quot;:</para>
          <screen># sysctl -w lnet.debug=&quot;+neterror +ha&quot; 
lnet.debug = +neterror +ha
# sysctl lnet.debug 
lnet.debug = neterror warning ha</screen>
        </listitem>
        <listitem>
          <para>To remove individual flags, precede them with a
            &quot;<literal>-</literal>&quot;:</para>
          <screen># sysctl -w lnet.debug=&quot;-ha&quot; 
lnet.debug = -ha
# sysctl lnet.debug 
lnet.debug = neterror warning</screen>
        </listitem>
        <listitem>
          <para>To verify or change the debug level, run commands such as the following: :</para>
          <screen># lctl get_param debug
debug=
neterror warning
# lctl set_param debug=+ha
# lctl get_param debug
debug=
neterror warning ha
# lctl set_param debug=-warning
# lctl get_param debug
debug=
neterror ha</screen>
        </listitem>
      </itemizedlist></para>
    <para>Debugging parameters include:</para>
    <itemizedlist>
      <listitem>
        <para><literal>subsystem_debug</literal> - Controls the debug logs for subsystems.</para>
      </listitem>
      <listitem>
        <para><literal>debug_path</literal> - Indicates the location where the debug log is dumped
          when triggered automatically or manually. The default path is
            <literal>/tmp/lustre-log</literal>.</para>
      </listitem>
    </itemizedlist>
    <para>These parameters are also set using:<screen>sysctl -w lnet.debug={value}</screen></para>
    <para>Additional useful parameters: <itemizedlist>
        <listitem>
          <para><literal>panic_on_lbug</literal> - Causes &apos;&apos;panic&apos;&apos; to be called
            when the Lustre software detects an internal problem (an <literal>LBUG</literal> log
            entry); panic crashes the node. This is particularly useful when a kernel crash dump
            utility is configured. The crash dump is triggered when the internal inconsistency is
            detected by the Lustre software. </para>
        </listitem>
        <listitem>
          <para><literal>upcall</literal> - Allows you to specify the path to the binary which will
            be invoked when an <literal>LBUG</literal> log entry is encountered. This binary is
            called with four parameters:</para>
          <para> - The string &apos;&apos;<literal>LBUG</literal>&apos;&apos;.</para>
          <para> - The file where the <literal>LBUG</literal> occurred.</para>
          <para> - The function name.</para>
          <para> - The line number in the file</para>
        </listitem>
      </itemizedlist></para>
    <section>
      <title>Interpreting OST Statistics</title>
      <note>
        <para>See also <xref linkend="dbdoclet.50438219_84890"/> (<literal>llobdstat</literal>) and
            <xref linkend="dbdoclet.50438273_80593"/> (<literal>collectl</literal>).</para>
      </note>
      <para>OST <literal>stats</literal> files can be used to provide statistics showing activity
        for each OST. For example:</para>
      <screen># lctl get_param osc.testfs-OST0000-osc.stats 
snapshot_time                      1189732762.835363
ost_create                 1
ost_get_info               1
ost_connect                1
ost_set_info               1
obd_ping                   212</screen>
      <para>Use the <literal>llstat</literal> utility to monitor statistics over time.</para>
      <para>To clear the statistics, use the <literal>-c</literal> option to
          <literal>llstat</literal>. To specify how frequently the statistics should be reported (in
        seconds), use the <literal>-i</literal> option. In the example below, the
          <literal>-c</literal> option clears the statistics and <literal>-i10</literal> option
        reports statistics every 10 seconds:</para>
      <screen role="smaller">$ llstat -c -i10 /proc/fs/lustre/ost/OSS/ost_io/stats
 
/usr/bin/llstat: STATS on 06/06/07 
        /proc/fs/lustre/ost/OSS/ost_io/ stats on 192.168.16.35@tcp
snapshot_time                              1181074093.276072
 
/proc/fs/lustre/ost/OSS/ost_io/stats @ 1181074103.284895
Name        Cur.  Cur. #
            Count Rate Events Unit  last   min    avg       max    stddev
req_waittime 8    0    8    [usec]  2078   34     259.75    868    317.49
req_qdepth   8    0    8    [reqs]  1      0      0.12      1      0.35
req_active   8    0    8    [reqs]  11     1      1.38      2      0.52
reqbuf_avail 8    0    8    [bufs]  511    63     63.88     64     0.35
ost_write    8    0    8    [bytes] 169767 72914  212209.62 387579 91874.29
 
/proc/fs/lustre/ost/OSS/ost_io/stats @ 1181074113.290180
Name        Cur.  Cur. #
            Count Rate Events Unit  last    min   avg       max    stddev
req_waittime 31   3    39   [usec]  30011   34    822.79    12245  2047.71
req_qdepth   31   3    39   [reqs]  0       0     0.03      1      0.16
req_active   31   3    39   [reqs]  58      1     1.77      3      0.74
reqbuf_avail 31   3    39   [bufs]  1977    63    63.79     64     0.41
ost_write    30   3    38   [bytes] 1028467 15019 315325.16 910694 197776.51
 
/proc/fs/lustre/ost/OSS/ost_io/stats @ 1181074123.325560
Name        Cur.  Cur. #
            Count Rate Events Unit  last    min    avg       max    stddev
req_waittime 21   2    60   [usec]  14970   34     784.32    12245  1878.66
req_qdepth   21   2    60   [reqs]  0       0      0.02      1      0.13
req_active   21   2    60   [reqs]  33      1      1.70      3      0.70
reqbuf_avail 21   2    60   [bufs]  1341    63     63.82     64     0.39
ost_write    21   2    59   [bytes] 7648424 15019  332725.08 910694 180397.87
</screen>
      <para>The columns in this example are described in the table below.</para>
      <informaltable frame="all">
        <tgroup cols="2">
          <colspec colname="c1" colwidth="50*"/>
          <colspec colname="c2" colwidth="50*"/>
          <thead>
            <row>
              <entry>
                <para><emphasis role="bold">Parameter</emphasis></para>
              </entry>
              <entry>
                <para><emphasis role="bold">Description</emphasis></para>
              </entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry><literal>Name</literal></entry>
              <entry>Name of the service event.  See the tables below for descriptions of service
                events that are tracked.</entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal>Cur. Count </literal></para>
              </entry>
              <entry>
                <para>Number of events of each type sent in the last interval.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal>Cur. Rate </literal></para>
              </entry>
              <entry>
                <para>Number of events per second in the last interval.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal> # Events </literal></para>
              </entry>
              <entry>
                <para>Total number of such events since the events have been cleared.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal> Unit </literal></para>
              </entry>
              <entry>
                <para>Unit of measurement for that statistic (microseconds, requests,
                  buffers).</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal> last </literal></para>
              </entry>
              <entry>
                <para>Average rate of these events (in units/event) for the last interval during
                  which they arrived. For instance, in the above mentioned case of
                    <literal>ost_destroy</literal> it took an average of 736 microseconds per
                  destroy for the 400 object destroys in the previous 10 seconds.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal> min </literal></para>
              </entry>
              <entry>
                <para>Minimum rate (in units/events) since the service started.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal> avg </literal></para>
              </entry>
              <entry>
                <para>Average rate.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal> max </literal></para>
              </entry>
              <entry>
                <para>Maximum rate.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal> stddev </literal></para>
              </entry>
              <entry>
                <para>Standard deviation (not measured in some cases)</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>
      <para>Events common to all services are shown in the table below.</para>
      <informaltable frame="all">
        <tgroup cols="2">
          <colspec colname="c1" colwidth="50*"/>
          <colspec colname="c2" colwidth="50*"/>
          <thead>
            <row>
              <entry>
                <para><emphasis role="bold">Parameter</emphasis></para>
              </entry>
              <entry>
                <para><emphasis role="bold">Description</emphasis></para>
              </entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>
                <para>
                  <literal> req_waittime </literal></para>
              </entry>
              <entry>
                <para>Amount of time a request waited in the queue before being handled by an
                  available server thread.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal> req_qdepth </literal></para>
              </entry>
              <entry>
                <para>Number of requests waiting to be handled in the queue for this service.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal> req_active </literal></para>
              </entry>
              <entry>
                <para>Number of requests currently being handled.</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal> reqbuf_avail </literal></para>
              </entry>
              <entry>
                <para>Number of unsolicited lnet request buffers for this service.</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>
      <para>Some service-specific events of interest are described in the table below.</para>
      <informaltable frame="all">
        <tgroup cols="2">
          <colspec colname="c1" colwidth="50*"/>
          <colspec colname="c2" colwidth="50*"/>
          <thead>
            <row>
              <entry>
                <para><emphasis role="bold">Parameter</emphasis></para>
              </entry>
              <entry>
                <para><emphasis role="bold">Description</emphasis></para>
              </entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>
                <para>
                  <literal> ldlm_enqueue </literal></para>
              </entry>
              <entry>
                <para>Time it takes to enqueue a lock (this includes file open on the MDS)</para>
              </entry>
            </row>
            <row>
              <entry>
                <para>
                  <literal> mds_reint </literal></para>
              </entry>
              <entry>
                <para>Time it takes to process an MDS modification record (includes
                    <literal>create</literal>, <literal>mkdir</literal>, <literal>unlink</literal>,
                    <literal>rename</literal> and <literal>setattr</literal>)</para>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>
    </section>
    <section>
      <title>Interpreting MDT Statistics</title>
      <note>
        <para>See also <xref linkend="dbdoclet.50438219_84890"/> (<literal>llobdstat</literal>) and
            <xref linkend="dbdoclet.50438273_80593"/> (<literal>collectl</literal>).</para>
      </note>
      <para>MDT <literal>stats</literal> files can be used to track MDT
      statistics for the MDS. The example below shows sample output from an
      MDT <literal>stats</literal> file.</para>
      <screen># lctl get_param mds.*-MDT0000.stats
snapshot_time                   1244832003.676892 secs.usecs 
open                            2 samples [reqs]
close                           1 samples [reqs]
getxattr                        3 samples [reqs]
process_config                  1 samples [reqs]
connect                         2 samples [reqs]
disconnect                      2 samples [reqs]
statfs                          3 samples [reqs]
setattr                         1 samples [reqs]
getattr                         3 samples [reqs]
llog_init                       6 samples [reqs] 
notify                          16 samples [reqs]</screen>
    </section>
  </section>
</chapter>
